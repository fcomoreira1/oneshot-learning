{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emnist in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: numpy in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (1.23.5)\n",
      "Requirement already satisfied: requests in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (2.29.0)\n",
      "Requirement already satisfied: tqdm in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (2023.5.7)\n",
      "Requirement already satisfied: tensorflow_datasets in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: absl-py in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: array-record in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.2.0)\n",
      "Requirement already satisfied: click in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.3)\n",
      "Requirement already satisfied: dm-tree in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.3.0)\n",
      "Requirement already satisfied: numpy in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (1.23.5)\n",
      "Requirement already satisfied: promise in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (4.23.1)\n",
      "Requirement already satisfied: psutil in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (2.29.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.13.1)\n",
      "Requirement already satisfied: termcolor in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: importlib_resources in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.5.7)\n",
      "Requirement already satisfied: six in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.59.0)\n",
      "Requirement already satisfied: opencv_python in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from opencv_python) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: chardet in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (5.1.0)\n",
      "Requirement already satisfied: pandas in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip show tensorflow_datasets\n",
    "!pip3 install emnist\n",
    "!pip3 install tensorflow_datasets\n",
    "!pip3 install opencv_python\n",
    "!pip3 install matplotlib\n",
    "!pip3 install chardet\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D,Conv2DTranspose, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline\n",
    "# tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Default GPU Device: /device:GPU:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), info = tfds.load('omniglot', split=['train', 'test'], with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tfds.as_dataframe(ds_train, info)\n",
    "df_test  = tfds.as_dataframe(ds_test, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 105, 105, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.stack(df_train['image'])\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alphabet', 'alphabet_char_id', 'image', 'label'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[['alphabet_char_id', 'label']].loc[np.where((df_train['alphabet'] == 27) & (df_train['alphabet_char_id'] == 23))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data handling general functions '''\n",
    "\n",
    "def separate_fewshot(test_images, test_labels, n=1):\n",
    "    oneshot_data = []\n",
    "    classify_data = []\n",
    "    for label in np.unique(test_labels):\n",
    "        for num in np.random.choice(np.where(test_labels == label)[0], n, False):\n",
    "            oneshot_data.append(num)\n",
    "    temp = set(oneshot_data)\n",
    "    for i in range(len(test_labels)):\n",
    "        if not i in temp: classify_data.append(i)\n",
    "    oneshot_images = test_images[oneshot_data]\n",
    "    oneshot_labels = test_labels[oneshot_data]\n",
    "    classify_images = test_images[classify_data]\n",
    "    classify_labels = test_labels[classify_data]\n",
    "    return oneshot_images, oneshot_labels, classify_images, classify_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, size, to_grayscale = True):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        resized_image = cv2.resize(img, (size, size))\n",
    "        if to_grayscale:\n",
    "            resized_image= cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        resized_images.append(resized_image)\n",
    "    return np.array(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_omniglot_dataframe(df, img_size = 56, reshape=True):\n",
    "    images = resize_images(df['image'], img_size)\n",
    "    if reshape:\n",
    "        images = images.reshape(-1, img_size * img_size)\n",
    "    else:\n",
    "        images.reshape(-1, img_size, img_size)\n",
    "        images = images[..., np.newaxis]\n",
    "    labels = df['label'].to_numpy()\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_dataset(\n",
    "        self, batch_size, repetitions, shots, num_classes, split=False\n",
    "    ):\n",
    "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
    "        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n",
    "        if split:\n",
    "            test_labels = np.zeros(shape=(num_classes))\n",
    "            test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n",
    "\n",
    "        # Get a random subset of labels from the entire label set.\n",
    "        label_subset = random.choices(self.labels, k=num_classes)\n",
    "        for class_idx, class_obj in enumerate(label_subset):\n",
    "            # Use enumerated index value as a temporary label for mini-batch in\n",
    "            # few shot learning.\n",
    "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
    "            # If creating a split dataset for testing, select an extra sample from each\n",
    "            # label to create the test dataset.\n",
    "            if split:\n",
    "                test_labels[class_idx] = class_idx\n",
    "                images_to_split = random.choices(\n",
    "                    self.data[label_subset[class_idx]], k=shots + 1\n",
    "                )\n",
    "                test_images[class_idx] = images_to_split[-1]\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = images_to_split[:-1]\n",
    "            else:\n",
    "                # For each index in the randomly selected label_subset, sample the\n",
    "                # necessary number of images.\n",
    "                temp_images[\n",
    "                    class_idx * shots : (class_idx + 1) * shots\n",
    "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
    "        )\n",
    "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
    "        if split:\n",
    "            return dataset, test_images, test_labels\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13180, 56, 56, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lbl = parse_omniglot_dataframe(df_test, reshape=False)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.1146074594680936\n",
      "======= DO NOTHING method: Finished =======\n",
      "0.1146074594680936\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "total = 0\n",
    "img_size = 56\n",
    "verbose = True\n",
    "(train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size)\n",
    "(test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size)\n",
    "t_alphabets = df_test['alphabet'].to_numpy()\n",
    "\n",
    "for alphabet in np.unique(t_alphabets):\n",
    "    ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "    labels = test_labels[ind_alphabet]\n",
    "    images = test_images[ind_alphabet]\n",
    "    os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=1)\n",
    "\n",
    "    #os_img = encoder.predict(os_img)\n",
    "    #clas_img = encoder.predict(clas_img)\n",
    "\n",
    "    #if verbose: print(\"Learning oneshot ...\")\n",
    "    nn = 1\n",
    "    neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "    neigh.fit(os_img, os_label)\n",
    "\n",
    "    #if verbose: print(\"Predicting ...\")\n",
    "    pred = neigh.predict(clas_img)\n",
    "\n",
    "    matches += np.sum(pred == clas_label)\n",
    "    total += len(clas_label)\n",
    "\n",
    "if verbose:\n",
    "    print(\"Accuracy: \", matches/total)\n",
    "    print(f\"======= DO NOTHING method: Finished =======\")\n",
    "\n",
    "print(matches/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PCA(df_train, df_test, img_size=56, n=1, n_components = 32, verbose=False, train=1):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    if verbose: print(\"======= PCA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X=train_images)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = pca.transform(os_img)\n",
    "        clas_img = pca.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= PCA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= PCA method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Vectorizing ...\n",
      "Accuracy:  0.17067326890823417\n",
      "======= PCA method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17067326890823417"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_PCA(df_train, df_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LDA(df_train, df_test, img_size=56, n=1, n_components = 32, verbose=False, train=1, c=3):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    # unique_labels = df_train['label'].unique()\n",
    "    # subsample_index = []\n",
    "    # for label in unique_labels:\n",
    "    #     for ind in np.random.choice(np.where(df_train['label'] == label)[0], c, False):\n",
    "    #         subsample_index.append(ind)\n",
    "    # subsample_index = np.array(subsample_index)\n",
    "    # train_images = train_images[subsample_index]\n",
    "    # train_labels = train_labels[subsample_index]\n",
    "    \n",
    "    if verbose: print(\"======= LDA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    lda = LDA(n_components=n_components)\n",
    "    lda.fit(X=train_images,y=train_labels)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = lda.transform(os_img)\n",
    "        clas_img = lda.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= LDA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= LDA method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Vectorizing ...\n",
      "Accuracy:  0.15813433431834517\n",
      "======= LDA method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15813433431834517"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_LDA(df_train, df_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_autoencoder(input_shape, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    input_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    encoder = keras.Sequential([\n",
    "        Flatten(),\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(input_size//8, activation='ReLU'),\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size//8, activation='ReLU'),\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_autoencoder(input_shape, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    input_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    encoder = keras.Sequential([\n",
    "        Flatten(),\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_autoencoder(input_shape, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "\n",
    "    :param int or tuple input_size: shape of the input samples\n",
    "    :param int code_size: dimension on which to project the original data\n",
    "    :return: autoencoder, encoder\n",
    "    \"\"\"\n",
    "    print(input_shape, code_size)\n",
    "    output_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    inputs = Input(shape=input_shape)\n",
    "    encoder = tf.keras.Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "    ])\n",
    "        \n",
    "    decoder = tf.keras.Sequential([\n",
    "        Dense(output_size // 8, activation='relu'),\n",
    "        Dense(output_size // 4, activation='relu'),\n",
    "        Dense(output_size // 2, activation='relu'),\n",
    "        Dense(output_size)\n",
    "    ])\n",
    "    autoencoder = Model(inputs=inputs, outputs=decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "    encoder = Model(inputs=inputs, outputs=encoder(inputs), name=\"encoder\")\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_autoencoder2(input_shape, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "\n",
    "    :param int or tuple input_size: shape of the input samples\n",
    "    :param int code_size: dimension on which to project the original data\n",
    "    :return: autoencoder, encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    print(input_shape, code_size)\n",
    "    encoder = tf.keras.Sequential([\n",
    "        Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        BatchNormalization(),\n",
    "    ])\n",
    "    \n",
    "    output_size = input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    conv_layer = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "    pool_layer = MaxPooling2D(pool_size=(2, 2))(conv_layer)\n",
    "    flatten_layer = Flatten()(pool_layer)\n",
    "    encoder = Dense(code_size)(flatten_layer)\n",
    "    dense_layer = Dense(output_size // 2)(encoder)\n",
    "    final_layer = Dense(output_size)(dense_layer)\n",
    "    \n",
    "    autoencoder = Model(inputs=inputs, outputs=final_layer, name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    encoder = Model(inputs=inputs, outputs=encoder, name=\"encoder\")\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder(df_train, df_test, autoencoder, img_size=56, num_components=32, n=1, verbose=False, train=1, epochs=50):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size, reshape=False)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size, reshape=False)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    #print(test_images.shape)\n",
    "    name = autoencoder.__name__\n",
    "    if verbose: print(f\"======= {name} Autoencoder method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    #print(test_images[0].shape)\n",
    "    autoencoder, encoder = autoencoder(test_images[0].shape, code_size=num_components)\n",
    "    print(autoencoder.summary())\n",
    "    autoencoder.fit(x=train_images,y=train_images.reshape(-1, img_size * img_size), epochs=epochs, batch_size=64)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        os_img = encoder.predict(os_img)\n",
    "        clas_img = encoder.predict(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(n, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(f\"======= {name} Autoencoder method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= nonlinear_autoencoder Autoencoder method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 56, 56, 1)]       0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 128)               2817432   \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 3136)              2820440   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,637,872\n",
      "Trainable params: 5,637,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnonlinear_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mtest_autoencoder\u001b[0;34m(df_train, df_test, autoencoder, img_size, num_components, n, verbose, train, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m autoencoder, encoder \u001b[38;5;241m=\u001b[39m autoencoder(test_images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, code_size\u001b[38;5;241m=\u001b[39mnum_components)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(autoencoder\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 14\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "test_autoencoder(df_train, df_test, num_components=128, epochs=1000, autoencoder=nonlinear_autoencoder, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
