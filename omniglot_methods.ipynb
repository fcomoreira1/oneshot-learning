{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emnist in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: numpy in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (1.23.5)\n",
      "Requirement already satisfied: requests in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (2.29.0)\n",
      "Requirement already satisfied: tqdm in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from emnist) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->emnist) (2023.5.7)\n",
      "Requirement already satisfied: tensorflow_datasets in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: absl-py in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: array-record in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.2.0)\n",
      "Requirement already satisfied: click in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.3)\n",
      "Requirement already satisfied: dm-tree in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.3.0)\n",
      "Requirement already satisfied: numpy in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (1.23.5)\n",
      "Requirement already satisfied: promise in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (4.23.1)\n",
      "Requirement already satisfied: psutil in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (2.29.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (1.13.1)\n",
      "Requirement already satisfied: termcolor in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: importlib_resources in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.5.7)\n",
      "Requirement already satisfied: six in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.59.0)\n",
      "Requirement already satisfied: opencv_python in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from opencv_python) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /users/eleves-a/2021/moreira.machado/.local/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: chardet in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (5.1.0)\n",
      "Requirement already satisfied: pandas in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /users/eleves-a/2021/moreira.machado/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip show tensorflow_datasets\n",
    "!pip3 install emnist\n",
    "!pip3 install tensorflow_datasets\n",
    "!pip3 install opencv_python\n",
    "!pip3 install matplotlib\n",
    "!pip3 install chardet\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline\n",
    "# tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Default GPU Device: /device:GPU:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), info = tfds.load('omniglot', split=['train', 'test'], with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tfds.as_dataframe(ds_train, info)\n",
    "df_test  = tfds.as_dataframe(ds_test, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19280, 105, 105, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.stack(df_train['image'])\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alphabet', 'alphabet_char_id', 'image', 'label'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[['alphabet_char_id', 'label']].loc[np.where((df_train['alphabet'] == 27) & (df_train['alphabet_char_id'] == 23))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data handling general functions '''\n",
    "\n",
    "def separate_fewshot(test_images, test_labels, n=1):\n",
    "    oneshot_data = []\n",
    "    classify_data = []\n",
    "    for label in np.unique(test_labels):\n",
    "        for num in np.random.choice(np.where(test_labels == label)[0], n, False):\n",
    "            oneshot_data.append(num)\n",
    "    temp = set(oneshot_data)\n",
    "    for i in range(len(test_labels)):\n",
    "        if not i in temp: classify_data.append(i)\n",
    "    oneshot_images = test_images[oneshot_data]\n",
    "    oneshot_labels = test_labels[oneshot_data]\n",
    "    classify_images = test_images[classify_data]\n",
    "    classify_labels = test_labels[classify_data]\n",
    "    return oneshot_images, oneshot_labels, classify_images, classify_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, size, to_grayscale = True):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        resized_image = cv2.resize(img, (size, size))\n",
    "        if to_grayscale:\n",
    "            resized_image= cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        resized_images.append(resized_image)\n",
    "    return np.array(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_omniglot_dataframe(df, img_size = 56):\n",
    "    images = resize_images(df['image'], img_size)\n",
    "    images = images.reshape(-1, img_size * img_size)\n",
    "    labels = df['label'].to_numpy()\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13180, 3136)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lbl = parse_omniglot_dataframe(df_test)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PCA(df_train, df_test, n=1, n_components = 32, verbose=False, train=1):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    if verbose: print(\"======= PCA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X=train_images)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = pca.transform(os_img)\n",
    "        clas_img = pca.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= PCA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_PCA(df_train, df_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LDA(df_train, df_test, n=1, n_components = 32, verbose=False, train=1, c=3):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    # unique_labels = df_train['label'].unique()\n",
    "    # subsample_index = []\n",
    "    # for label in unique_labels:\n",
    "    #     for ind in np.random.choice(np.where(df_train['label'] == label)[0], c, False):\n",
    "    #         subsample_index.append(ind)\n",
    "    # subsample_index = np.array(subsample_index)\n",
    "    # train_images = train_images[subsample_index]\n",
    "    # train_labels = train_labels[subsample_index]\n",
    "    \n",
    "    if verbose: print(\"======= LDA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    lda = LDA(n_components=n_components)\n",
    "    lda.fit(X=train_images,y=train_labels)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = lda.transform(os_img)\n",
    "        clas_img = lda.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= LDA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_LDA(df_train, df_test, verbose=True,c=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=(input_size,))\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def linear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=(input_size,))\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder(df_train, df_test, autoencoder, img_size=56, num_components=32, n=1, verbose=False, train=1):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "        \n",
    "    if verbose: print(\"======= NL Autoencoder method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    autoencoder, encoder = autoencoder(img_size * img_size, code_size=num_components)\n",
    "    autoencoder.fit(x=train_images,y=train_images, epochs=50, batch_size=64)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        os_img = encoder.predict(os_img)\n",
    "        clas_img = encoder.predict(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= NL Autoencoder method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_autoencoder(df_train, df_test, autoencoder=nonlinear_autoencoder, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_net_and_encoder(input_shape, code_size = 0):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv2D(64, (10, 10), input_shape=input_shape, activation='relu', kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2))\n",
    "    encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Conv2D(128, (7, 7), kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2))\n",
    "    encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Conv2D(128, (4, 4), kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2))\n",
    "    encoder.add(Dropout(0.25))\n",
    "    #encoder.add(Conv2D(256, (4, 4), kernel_regularizer='l2'))\n",
    "    \n",
    "    encoder.add(Flatten())\n",
    "    \n",
    "    encoder.add(Dense(2048, activation='sigmoid', kernel_regularizer='l2'))\n",
    "    \n",
    "    left_emb = encoder(left_input)\n",
    "    right_emb = encoder(right_input)\n",
    "    \n",
    "    L1_Layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_emb,right_emb])\n",
    "    OP = Dense(1, activation='sigmoid', kernel_regularizer='l2')(L1_Dist)\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=OP)\n",
    "    \n",
    "    return siamese_net, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19280, 56, 56)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 56, 56, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 56, 56, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 2048)         1722176     ['input_13[0][0]',               \n",
      "                                                                  'input_14[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 2048)         0           ['sequential_6[0][0]',           \n",
      "                                                                  'sequential_6[1][0]']           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            2049        ['lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,724,225\n",
      "Trainable params: 1,723,585\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 56\n",
    "\n",
    "(train_images, train_labels) = resize_images(df_train['image'], IMG_SIZE), df_train['label'].to_numpy()\n",
    "\n",
    "t_alphabets = df_test['alphabet'].to_numpy()\n",
    "\n",
    "print(train_images.shape)\n",
    "_, w, h = train_images.shape\n",
    "\n",
    "siamese_net, encoder = get_siamese_net_and_encoder((w, h, 1))\n",
    "\n",
    "siamese_net.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f826b622940>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f826b7c9c40>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(df_train['image'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_label(label, train_images, train_labels):\n",
    "    return train_images[np.random.choice(np.where(train_labels == label)[0], 1, False)[0]]\n",
    "\n",
    "def get_train_data(size, train_images, train_labels, img_size):\n",
    "    targets = np.zeros((size,))\n",
    "    targets[size // 2:] = 1\n",
    "    pairs = [np.zeros((size, img_size, img_size)) for _ in range(2)]\n",
    "    labels = np.unique(train_labels)\n",
    "    for i in range(size):\n",
    "        class1 = np.random.choice(labels, 1)[0]\n",
    "        class2 = class1\n",
    "        if i < size // 2:\n",
    "            while class2 == class1:\n",
    "                class2 = np.random.choice(labels, 1)[0]\n",
    "        pairs[0][i] = get_image_by_label(class1, train_images, train_labels)\n",
    "        pairs[1][i] = get_image_by_label(class2, train_images, train_labels)\n",
    "    return pairs, targets\n",
    "\n",
    "def get_train_batch(train_images, train_labels, n_classes, n_shots, n_support, n_iterations, img_size=56):\n",
    "    labels = np.unique(train_labels)\n",
    "    num_labels = labels.shape[0]\n",
    "    np.random.shuffle(labels)\n",
    "    \n",
    "    x, y = np.zeros((n_classes * n_shots, img_size, img_size)), np.zeros((n_classes * n_support, img_size, img_size))\n",
    "    x_l, y_l = np.zeros(n_classes * n_shots), np.zeros(n_classes * n_support)\n",
    "    \n",
    "    size = n_classes * n_classes * n_shots * n_support\n",
    "    n_iterations = min(n_iterations, (num_labels + n_classes - 1) // n_classes)\n",
    "    \n",
    "    pairs = [[None for _ in range(size * n_iterations)], [None for _ in range(size * n_iterations)]]\n",
    "    target = np.zeros(size * n_iterations)\n",
    "    \n",
    "    cur = 0\n",
    "    for t in range(0, n_iterations * n_classes, n_classes):\n",
    "        classes = labels[t: t + n_classes]\n",
    "        #classes = np.random.choice(labels, n_classes, False)        \n",
    "        for i, c in enumerate(classes):\n",
    "            ind = np.where(train_labels == c)[0]\n",
    "            ind = np.random.choice(ind, n_shots + n_support, False)\n",
    "\n",
    "            x_ind = ind[:n_shots]\n",
    "            x[i * n_shots: (i+1) * n_shots, :] = train_images[x_ind]\n",
    "            x_l[i * n_shots: (i+1) * n_shots] = train_labels[x_ind]\n",
    "\n",
    "            y_ind = ind[n_shots:]\n",
    "            y[i * n_support: (i+1) * n_support, :] = train_images[y_ind]\n",
    "            y_l[i * n_support: (i+1) * n_support] = train_labels[y_ind]\n",
    "        for i, xx in enumerate(x):\n",
    "            if x_l[i] not in classes:\n",
    "                continue\n",
    "            for j, yy in enumerate(y):\n",
    "                if y_l[j] not in classes:\n",
    "                    continue\n",
    "                pairs[0][cur] = xx\n",
    "                pairs[1][cur] = yy\n",
    "                target[cur] = ((x_l[i] == y_l[j]) and (x_l[i] != -1))\n",
    "                cur += 1\n",
    "    pairs[0] = np.array(pairs[0][:cur])\n",
    "    pairs[1] = np.array(pairs[1][:cur])\n",
    "    target = target[:cur]\n",
    "    return pairs, target\n",
    "\n",
    "def visualize_siamese(siamese_net, train_images, train_labels, label1 = 1, label2=2):\n",
    "    x = train_images[np.where(train_labels == label1)[0][:1]]\n",
    "    y = train_images[np.where(train_labels == label2)[0][1:2]]\n",
    "    d = siamese_net.predict([x, y], verbose=0)\n",
    "    fig, axis = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axis[0].axis(\"off\")\n",
    "    axis[1].axis(\"off\")\n",
    "    fig.suptitle(f\"Output {d[0][0]}\")\n",
    "    axis[0].imshow(x[0], cmap='gray')\n",
    "    axis[1].imshow(y[0], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "num_iterations_alphabet = 1000\n",
    "batch_size = 62\n",
    "evaluateEvery = 200\n",
    "num_shots = 1\n",
    "IMG_SIZE = 56\n",
    "n_support = 5\n",
    "n_classes = 10\n",
    "num_labels = np.unique(train_labels).shape[0]\n",
    "n_iterations = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.1772 - accuracy: 0.9935\n",
      "Epoch 2:\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.2005 - accuracy: 0.9914\n",
      "Epoch 3:\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.2584 - accuracy: 0.9903\n",
      "Epoch 4:\n",
      "242/242 [==============================] - 5s 21ms/step - loss: 0.2909 - accuracy: 0.9864\n",
      "Epoch 5:\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m siamese_net\u001b[38;5;241m.\u001b[39mfit(x, y, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m#if i % evaluateEvery == 0:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m#    print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_one_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[172], line 7\u001b[0m, in \u001b[0;36mtrain_one_shot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m get_train_batch(train_images, train_labels, n_classes\u001b[38;5;241m=\u001b[39mn_classes, n_shots\u001b[38;5;241m=\u001b[39mnum_shots, n_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_iterations\u001b[38;5;241m=\u001b[39mn_iterations, img_size\u001b[38;5;241m=\u001b[39mIMG_SIZE)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(\"shapes:\", x[0].shape, x[1].shape, y.shape)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(\"sum is:\", np.sum(y), \"should be:\", n_iterations * n_classes * num_shots * n_support)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "def train_one_shot():\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        print(f\"Epoch {i}:\")\n",
    "        x, y = get_train_batch(train_images, train_labels, n_classes=n_classes, n_shots=num_shots, n_support=5, n_iterations=n_iterations, img_size=IMG_SIZE)\n",
    "        #print(\"shapes:\", x[0].shape, x[1].shape, y.shape)\n",
    "        #print(\"sum is:\", np.sum(y), \"should be:\", n_iterations * n_classes * num_shots * n_support)\n",
    "        loss = siamese_net.fit(x, y, batch_size=batch_size,)\n",
    "        #if i % evaluateEvery == 0:\n",
    "        #    print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))\n",
    "train_one_shot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_alphabet():\n",
    "    alphabets = np.unique(df_train['alphabet'])\n",
    "    info_alphabet = {}\n",
    "    for alphabet in alphabets:\n",
    "        ind = np.where(df_train['alphabet'] == alphabet)[0]\n",
    "        #print(train_images[ind].shape, ind.shape, train_images.shape)\n",
    "        info_alphabet[alphabet] = (train_images[ind], train_labels[ind])\n",
    "\n",
    "    for i in range(1, num_iterations_alphabet + 1):\n",
    "        loss = 0\n",
    "        for alphabet in alphabets:\n",
    "            ia = info_alphabet[alphabet]\n",
    "            x, y = get_train_data(batch_size, ia[0], ia[1], IMG_SIZE)\n",
    "            loss += siamese_net.train_on_batch(x, y)[0]\n",
    "        if i % 10 == 0:\n",
    "            print('Iteration', i, '- Loss:',loss)\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        x, y = get_train_data(batch_size, train_images, train_labels, IMG_SIZE)\n",
    "        loss = siamese_net.train_on_batch(x, y)\n",
    "        if i % evaluateEvery == 0:\n",
    "            print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_batch()\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        x, y = get_train_data(batch_size, train_images, train_labels, IMG_SIZE)\n",
    "        loss = siamese_net.train_on_batch(x, y)\n",
    "        if i % evaluateEvery == 0:\n",
    "            print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_siamese(siamese_net, train_images, train_labels, label1=63, label2=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing ...\n",
      "20 380\n",
      "(7600, 56, 56) (7600, 56, 56)\n",
      "Projecting alphabet 1\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 6\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 7\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 8\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 9\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 18\n",
      "Testing\n",
      "45 855\n",
      "(38475, 56, 56) (38475, 56, 56)\n",
      "Projecting alphabet 19\n",
      "Testing\n",
      "45 855\n",
      "(38475, 56, 56) (38475, 56, 56)\n",
      "Projecting alphabet 23\n",
      "Testing\n",
      "41 779\n",
      "(31939, 56, 56) (31939, 56, 56)\n",
      "Projecting alphabet 28\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 29\n",
      "Testing\n",
      "47 893\n",
      "(41971, 56, 56) (41971, 56, 56)\n",
      "Projecting alphabet 33\n",
      "Testing\n",
      "40 760\n",
      "(30400, 56, 56) (30400, 56, 56)\n",
      "Projecting alphabet 34\n",
      "Testing\n",
      "30 570\n",
      "(17100, 56, 56) (17100, 56, 56)\n",
      "Projecting alphabet 36\n",
      "Testing\n",
      "45 855\n",
      "(38475, 56, 56) (38475, 56, 56)\n",
      "Projecting alphabet 39\n",
      "Testing\n",
      "46 874\n",
      "(40204, 56, 56) (40204, 56, 56)\n",
      "Projecting alphabet 40\n",
      "Testing\n",
      "28 532\n",
      "(14896, 56, 56) (14896, 56, 56)\n",
      "Projecting alphabet 42\n",
      "Testing\n",
      "23 437\n",
      "(10051, 56, 56) (10051, 56, 56)\n",
      "Projecting alphabet 44\n",
      "Testing\n",
      "25 475\n",
      "(11875, 56, 56) (11875, 56, 56)\n",
      "Projecting alphabet 46\n",
      "Testing\n",
      "42 798\n",
      "(33516, 56, 56) (33516, 56, 56)\n",
      "Projecting alphabet 47\n",
      "Testing\n",
      "26 494\n",
      "(12844, 56, 56) (12844, 56, 56)\n",
      "Projecting alphabet 49\n",
      "Testing\n",
      "Accuracy:  0.14751217953837553\n",
      "======= NL Autoencoder method: Finished =======\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "total = 0\n",
    "(test_images, test_labels) = resize_images(df_test['image'], IMG_SIZE), df_test['label'].to_numpy()\n",
    "\n",
    "def get_mode(l):\n",
    "    d = {}\n",
    "    mode = None\n",
    "    count = 0\n",
    "    for i in l:\n",
    "        if i in d:\n",
    "            d[i] += 1\n",
    "        else:\n",
    "            d[i] = 1\n",
    "        if d[i] > count:\n",
    "            mode = i\n",
    "    return mode\n",
    "\n",
    "print(\"Vectorizing ...\")\n",
    "for alphabet in np.unique(df_test['alphabet']):\n",
    "    ind_alphabet = np.where(df_test['alphabet'] == alphabet)[0]\n",
    "    labels = test_labels[ind_alphabet]\n",
    "    images = test_images[ind_alphabet]\n",
    "    os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=num_shots)\n",
    "    \n",
    "    l_os = os_img\n",
    "    l_clas = clas_img\n",
    "    print(len(l_os), len(l_clas))\n",
    "    x, y = [], []\n",
    "    for i, c_i in enumerate(l_clas):\n",
    "        x += [c_i.copy() for _ in range(len(l_os))]\n",
    "        y = [*y, *l_os]\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    print(x.shape, y.shape)\n",
    "    print(f\"Projecting alphabet {alphabet}\")\n",
    "    t = siamese_net.predict([x, y], verbose=0)\n",
    "    print(\"Testing\")\n",
    "    for i in range(len(clas_img)):\n",
    "        arr = t[i * len(os_img): (i + 1) * len(os_img)].reshape(-1)\n",
    "        ind = np.argsort(arr)[-num_shots:]\n",
    "        ind = get_mode(ind)\n",
    "        pred = os_label[ind]\n",
    "        #print(t.shape, ind, pred, clas_label[i])\n",
    "        matches += np.sum(pred == clas_label[i])\n",
    "        total += 1\n",
    "\n",
    "print(\"Accuracy: \", matches/total)\n",
    "print(\"======= NL Autoencoder method: Finished =======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying the encoder with 1 shots\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "15/15 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "0.14910949604664164\n"
     ]
    }
   ],
   "source": [
    "matches = total = 0\n",
    "print(f\"Trying the encoder with {num_shots} shots\")\n",
    "for alphabet in np.unique(df_test['alphabet']):\n",
    "    ind_alphabet = np.where(df_test['alphabet'] == alphabet)[0]\n",
    "    labels = test_labels[ind_alphabet]\n",
    "    images = test_images[ind_alphabet]\n",
    "    os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=num_shots)\n",
    "    \n",
    "    os_img = encoder.predict(os_img)\n",
    "    clas_img = encoder.predict(clas_img)\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors = num_shots)\n",
    "    neigh.fit(os_img, os_label)\n",
    "    \n",
    "    pred = neigh.predict(clas_img)\n",
    "    matches += np.sum(pred == clas_label)\n",
    "    total += len(clas_label)\n",
    "print(matches/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total = matches = 0\n",
    "os_img, os_label, clas_img, clas_label = separate_fewshot(test_images, test_labels, n=1)\n",
    "os_img = encoder.predict(os_img)\n",
    "clas_img = encoder.predict(clas_img)\n",
    "\n",
    "#if verbose: print(\"Learning oneshot ...\")\n",
    "#nn = min(train, 5)\n",
    "neigh = KNeighborsClassifier(n_neighbors = 1)\n",
    "neigh.fit(os_img, os_label)\n",
    "\n",
    "#if verbose: print(\"Predicting ...\")\n",
    "pred = neigh.predict(clas_img)\n",
    "\n",
    "matches += np.sum(pred == clas_label)\n",
    "total += len(clas_label)\n",
    "print(matches / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
