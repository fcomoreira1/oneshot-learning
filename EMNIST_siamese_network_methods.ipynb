{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import methods as M\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Loading emnist data ... =======\n",
      "Output shape:  [(96000, 28, 28), (96000,), (16800, 28, 28), (16800,)]\n",
      "Train labels:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 18 19 22 24 25 26 27 28\n",
      " 29 30 32 33 34 35 36 37 38 40 41 42 43 44 45 46]\n",
      "Test labels:  [14 17 20 21 23 31 39]\n",
      "======= Finished loading. =======\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, oneshot_images, oneshot_labels, classify_images, classify_labels = M.get_emnist(40, 1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_label(label):\n",
    "    return train_images[np.random.choice(np.where(train_labels == label)[0], 1, False)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(size):\n",
    "    targets = np.zeros((size,))\n",
    "    targets[size//2:] = 1\n",
    "    pairs = [np.zeros((size, 28, 28)) for _ in range(2)]\n",
    "    labels = np.unique(train_labels)\n",
    "    for i in range(size):\n",
    "        class1 = np.random.choice(labels, 1)[0]\n",
    "        class2 = class1\n",
    "        if i < size//2:\n",
    "            while class2 == class1:\n",
    "                class2 = np.random.choice(labels, 1)[0]\n",
    "        pairs[0][i] = get_image_by_label(class1)\n",
    "        pairs[1][i] = get_image_by_label(class2)\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_net_and_encoder(input_shape):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv2D(16, (3, 3), input_shape=input_shape, activation='relu', kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    # encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Conv2D(32, (3, 3), kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    # encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Flatten())\n",
    "    \n",
    "    encoder.add(Dense(32, activation='sigmoid', kernel_regularizer='l2'))\n",
    "    \n",
    "    left_emb = encoder(left_input)\n",
    "    right_emb = encoder(right_input)\n",
    "    \n",
    "    L1_Layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_emb,right_emb])\n",
    "    OP = Dense(1, activation='sigmoid', kernel_regularizer='l2')(L1_Dist)\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=OP)\n",
    "    \n",
    "    return siamese_net, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_shot_task(N):\n",
    "    labels = np.unique(train_labels)\n",
    "    cats = np.random.choice(labels, N, replace=False)\n",
    "    _, w, h = train_images.shape\n",
    "    true_cat = cats[0]\n",
    "    test_image = np.array([get_image_by_label(true_cat)]*N).reshape(N, w, h, 1)\n",
    "    support_set = np.array([get_image_by_label(cat) for cat in cats]).reshape(N, w, h, 1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image,support_set,targets = sklearn.utils.shuffle(test_image,support_set,targets)\n",
    "    \n",
    "    return [test_image,support_set], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_shot(model, N, k):\n",
    "    n_correct = 0\n",
    "    for _ in range(k):\n",
    "        inputs, outputs = make_one_shot_task(N)\n",
    "        preds = model.predict(inputs)\n",
    "        if np.argmax(outputs) == np.argmax(preds):\n",
    "            n_correct += 1\n",
    "    return n_correct / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_siamese(siamese_net, encoder, similar = None):\n",
    "    if similar == None: \n",
    "        if np.random.rand() < 1/2: return test_siamese(siamese_net, encoder, True)\n",
    "        else: return test_siamese(siamese_net, encoder, False)\n",
    "    \n",
    "    labels = np.unique(train_labels)\n",
    "    class1 = np.random.choice(labels, 1)[0]\n",
    "    class2 = class1\n",
    "    if not similar:\n",
    "        while class2 == class1:\n",
    "            class2 = np.random.choice(labels, 1)[0]\n",
    "    img1 = np.expand_dims(get_image_by_label(class1), axis=-1)\n",
    "    img2 = np.expand_dims(get_image_by_label(class2), axis=-1)\n",
    "    print(\"======= Testing siamese network ... =======\")\n",
    "    print(\"Class 1: \", class1)\n",
    "    print(\"Class 2: \", class2)\n",
    "    print(\"Shape: \", img1.shape)\n",
    "\n",
    "    f, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    axes[0].imshow(img1)\n",
    "    axes[1].imshow(img2)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    print(\"Similarity: \", siamese_net.predict([[img1, img2]]))\n",
    "    return siamese_net.predict([[img1, img2]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_41 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_21 (Sequential)     (None, 32)           30624       ['input_40[0][0]',               \n",
      "                                                                  'input_41[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_17 (Lambda)             (None, 32)           0           ['sequential_21[0][0]',          \n",
      "                                                                  'sequential_21[1][0]']          \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 1)            33          ['lambda_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,657\n",
      "Trainable params: 30,561\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5000\n",
    "batch_size = 20\n",
    "evaluateEvery = 10\n",
    "N = 20\n",
    "k = 250\n",
    "\n",
    "_, w, h = train_images.shape\n",
    "\n",
    "siamese_net, encoder = get_siamese_net_and_encoder((w, h, 1))\n",
    "\n",
    "siamese_net.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 1.547972321510315 - Acc: 0.65\n",
      "Iteration 10 - Loss: 1.4393503665924072 - Acc: 0.5\n",
      "Iteration 20 - Loss: 1.410040259361267 - Acc: 0.55\n",
      "Iteration 30 - Loss: 1.308828592300415 - Acc: 0.55\n",
      "Iteration 40 - Loss: 1.2289851903915405 - Acc: 0.7\n",
      "Iteration 50 - Loss: 1.1834139823913574 - Acc: 0.6\n",
      "Iteration 60 - Loss: 1.144364833831787 - Acc: 0.6\n",
      "Iteration 70 - Loss: 1.0801550149917603 - Acc: 0.65\n",
      "Iteration 80 - Loss: 0.982525646686554 - Acc: 0.95\n",
      "Iteration 90 - Loss: 1.0013208389282227 - Acc: 0.75\n",
      "Iteration 100 - Loss: 1.0163426399230957 - Acc: 0.75\n",
      "Iteration 110 - Loss: 1.0005203485488892 - Acc: 0.65\n",
      "Iteration 120 - Loss: 0.9577542543411255 - Acc: 0.75\n",
      "Iteration 130 - Loss: 0.9809759855270386 - Acc: 0.6\n",
      "Iteration 140 - Loss: 0.9494973421096802 - Acc: 0.65\n",
      "Iteration 150 - Loss: 0.8452537655830383 - Acc: 0.7\n",
      "Iteration 160 - Loss: 0.853649914264679 - Acc: 0.75\n",
      "Iteration 170 - Loss: 0.8929784893989563 - Acc: 0.7\n",
      "Iteration 180 - Loss: 0.8623771667480469 - Acc: 0.75\n",
      "Iteration 190 - Loss: 0.801590085029602 - Acc: 0.75\n",
      "Iteration 200 - Loss: 0.7982676029205322 - Acc: 0.75\n",
      "Iteration 210 - Loss: 0.8013315200805664 - Acc: 0.75\n",
      "Iteration 220 - Loss: 0.8302744030952454 - Acc: 0.7\n",
      "Iteration 230 - Loss: 0.8128522634506226 - Acc: 0.75\n",
      "Iteration 240 - Loss: 0.8159607648849487 - Acc: 0.8\n",
      "Iteration 250 - Loss: 0.7519519329071045 - Acc: 0.9\n",
      "Iteration 260 - Loss: 0.7451434135437012 - Acc: 0.85\n",
      "Iteration 270 - Loss: 0.7394996881484985 - Acc: 0.75\n",
      "Iteration 280 - Loss: 0.7046747207641602 - Acc: 0.85\n",
      "Iteration 290 - Loss: 0.7334141731262207 - Acc: 0.7\n",
      "Iteration 300 - Loss: 0.7598312497138977 - Acc: 0.65\n",
      "Iteration 310 - Loss: 0.696606457233429 - Acc: 0.8\n",
      "Iteration 320 - Loss: 0.6725555658340454 - Acc: 0.8\n",
      "Iteration 330 - Loss: 0.6443101763725281 - Acc: 0.85\n",
      "Iteration 340 - Loss: 0.7012079954147339 - Acc: 0.75\n",
      "Iteration 350 - Loss: 0.6433060169219971 - Acc: 0.9\n",
      "Iteration 360 - Loss: 0.6918011903762817 - Acc: 0.75\n",
      "Iteration 370 - Loss: 0.6751838326454163 - Acc: 0.85\n",
      "Iteration 380 - Loss: 0.6070416569709778 - Acc: 0.85\n",
      "Iteration 390 - Loss: 0.6876848936080933 - Acc: 0.75\n",
      "Iteration 400 - Loss: 0.7467054724693298 - Acc: 0.65\n",
      "Iteration 410 - Loss: 0.6335678100585938 - Acc: 0.8\n",
      "Iteration 420 - Loss: 0.6782644987106323 - Acc: 0.85\n",
      "Iteration 430 - Loss: 0.5721679925918579 - Acc: 0.9\n",
      "Iteration 440 - Loss: 0.6580197215080261 - Acc: 0.75\n",
      "Iteration 450 - Loss: 0.7029685378074646 - Acc: 0.75\n",
      "Iteration 460 - Loss: 0.6169528365135193 - Acc: 0.9\n",
      "Iteration 470 - Loss: 0.7265482544898987 - Acc: 0.85\n",
      "Iteration 480 - Loss: 0.6530279517173767 - Acc: 0.75\n",
      "Iteration 490 - Loss: 0.694488525390625 - Acc: 0.65\n",
      "Iteration 500 - Loss: 0.6913369297981262 - Acc: 0.7\n",
      "Iteration 510 - Loss: 0.618665337562561 - Acc: 0.85\n",
      "Iteration 520 - Loss: 0.6375411152839661 - Acc: 0.8\n",
      "Iteration 530 - Loss: 0.6268582940101624 - Acc: 0.85\n",
      "Iteration 540 - Loss: 0.538828432559967 - Acc: 0.9\n",
      "Iteration 550 - Loss: 0.598487377166748 - Acc: 0.85\n",
      "Iteration 560 - Loss: 0.5550916790962219 - Acc: 0.85\n",
      "Iteration 570 - Loss: 0.5877963900566101 - Acc: 0.8\n",
      "Iteration 580 - Loss: 0.5332818627357483 - Acc: 1.0\n",
      "Iteration 590 - Loss: 0.6250800490379333 - Acc: 0.9\n",
      "Iteration 600 - Loss: 0.6512532234191895 - Acc: 0.8\n",
      "Iteration 610 - Loss: 0.5373643636703491 - Acc: 0.85\n",
      "Iteration 620 - Loss: 0.596770167350769 - Acc: 0.9\n",
      "Iteration 630 - Loss: 0.6226891875267029 - Acc: 0.75\n",
      "Iteration 640 - Loss: 0.551160454750061 - Acc: 1.0\n",
      "Iteration 650 - Loss: 0.507857620716095 - Acc: 0.95\n",
      "Iteration 660 - Loss: 0.5970707535743713 - Acc: 0.9\n",
      "Iteration 670 - Loss: 0.6367864608764648 - Acc: 0.85\n",
      "Iteration 680 - Loss: 0.5838736891746521 - Acc: 0.85\n",
      "Iteration 690 - Loss: 0.5050062537193298 - Acc: 0.9\n",
      "Iteration 700 - Loss: 0.608474850654602 - Acc: 0.8\n",
      "Iteration 710 - Loss: 0.6067001223564148 - Acc: 0.8\n",
      "Iteration 720 - Loss: 0.5750614404678345 - Acc: 0.85\n",
      "Iteration 730 - Loss: 0.6242952346801758 - Acc: 0.8\n",
      "Iteration 740 - Loss: 0.7356465458869934 - Acc: 0.7\n",
      "Iteration 750 - Loss: 0.5871960520744324 - Acc: 0.9\n",
      "Iteration 760 - Loss: 0.555963397026062 - Acc: 0.9\n",
      "Iteration 770 - Loss: 0.6429890394210815 - Acc: 0.8\n",
      "Iteration 780 - Loss: 0.5613524317741394 - Acc: 0.9\n",
      "Iteration 790 - Loss: 0.6816607117652893 - Acc: 0.75\n",
      "Iteration 800 - Loss: 0.6858486533164978 - Acc: 0.75\n",
      "Iteration 810 - Loss: 0.5212804079055786 - Acc: 0.9\n",
      "Iteration 820 - Loss: 0.5723772048950195 - Acc: 0.9\n",
      "Iteration 830 - Loss: 0.6234185695648193 - Acc: 0.75\n",
      "Iteration 840 - Loss: 0.5838727951049805 - Acc: 0.85\n",
      "Iteration 850 - Loss: 0.5211905241012573 - Acc: 0.8\n",
      "Iteration 860 - Loss: 0.5799990892410278 - Acc: 0.75\n",
      "Iteration 870 - Loss: 0.5005812048912048 - Acc: 0.9\n",
      "Iteration 880 - Loss: 0.43564867973327637 - Acc: 1.0\n",
      "Iteration 890 - Loss: 0.6055511832237244 - Acc: 0.75\n",
      "Iteration 900 - Loss: 0.5936042666435242 - Acc: 0.8\n",
      "Iteration 910 - Loss: 0.665293276309967 - Acc: 0.8\n",
      "Iteration 920 - Loss: 0.5089337825775146 - Acc: 0.9\n",
      "Iteration 930 - Loss: 0.6072453856468201 - Acc: 0.7\n",
      "Iteration 940 - Loss: 0.5738306641578674 - Acc: 0.75\n",
      "Iteration 950 - Loss: 0.6504906415939331 - Acc: 0.85\n",
      "Iteration 960 - Loss: 0.4941806197166443 - Acc: 0.9\n",
      "Iteration 970 - Loss: 0.5575665831565857 - Acc: 0.8\n",
      "Iteration 980 - Loss: 0.5623199939727783 - Acc: 0.85\n",
      "Iteration 990 - Loss: 0.456450879573822 - Acc: 1.0\n",
      "Iteration 1000 - Loss: 0.4959380030632019 - Acc: 0.9\n",
      "Iteration 1010 - Loss: 0.6262456774711609 - Acc: 0.75\n",
      "Iteration 1020 - Loss: 0.5249365568161011 - Acc: 0.9\n",
      "Iteration 1030 - Loss: 0.5134270191192627 - Acc: 0.95\n",
      "Iteration 1040 - Loss: 0.4941207766532898 - Acc: 0.95\n",
      "Iteration 1050 - Loss: 0.6247352957725525 - Acc: 0.8\n",
      "Iteration 1060 - Loss: 0.7198989391326904 - Acc: 0.65\n",
      "Iteration 1070 - Loss: 0.6879924535751343 - Acc: 0.7\n",
      "Iteration 1080 - Loss: 0.5857822895050049 - Acc: 0.8\n",
      "Iteration 1090 - Loss: 0.45833200216293335 - Acc: 1.0\n",
      "Iteration 1100 - Loss: 0.5532673001289368 - Acc: 0.75\n",
      "Iteration 1110 - Loss: 0.5248844027519226 - Acc: 0.8\n",
      "Iteration 1120 - Loss: 0.5881516337394714 - Acc: 0.95\n",
      "Iteration 1130 - Loss: 0.5022249221801758 - Acc: 0.85\n",
      "Iteration 1140 - Loss: 0.559821605682373 - Acc: 0.8\n",
      "Iteration 1150 - Loss: 0.5976841449737549 - Acc: 0.75\n",
      "Iteration 1160 - Loss: 0.510853111743927 - Acc: 0.8\n",
      "Iteration 1170 - Loss: 0.610270619392395 - Acc: 0.8\n",
      "Iteration 1180 - Loss: 0.4922851622104645 - Acc: 0.9\n",
      "Iteration 1190 - Loss: 0.6377015113830566 - Acc: 0.85\n",
      "Iteration 1200 - Loss: 0.5128939747810364 - Acc: 0.8\n",
      "Iteration 1210 - Loss: 0.49429211020469666 - Acc: 0.95\n",
      "Iteration 1220 - Loss: 0.5735965371131897 - Acc: 0.8\n",
      "Iteration 1230 - Loss: 0.7886272072792053 - Acc: 0.75\n",
      "Iteration 1240 - Loss: 0.5403848886489868 - Acc: 0.85\n",
      "Iteration 1250 - Loss: 0.5190846920013428 - Acc: 0.9\n",
      "Iteration 1260 - Loss: 0.4694289565086365 - Acc: 0.95\n",
      "Iteration 1270 - Loss: 0.45910900831222534 - Acc: 0.95\n",
      "Iteration 1280 - Loss: 0.4993566870689392 - Acc: 0.9\n",
      "Iteration 1290 - Loss: 0.42200368642807007 - Acc: 1.0\n",
      "Iteration 1300 - Loss: 0.5883324146270752 - Acc: 0.75\n",
      "Iteration 1310 - Loss: 0.6460018157958984 - Acc: 0.8\n",
      "Iteration 1320 - Loss: 0.5253662467002869 - Acc: 0.9\n",
      "Iteration 1330 - Loss: 0.5062627196311951 - Acc: 0.9\n",
      "Iteration 1340 - Loss: 0.5329216718673706 - Acc: 0.85\n",
      "Iteration 1350 - Loss: 0.5417040586471558 - Acc: 0.9\n",
      "Iteration 1360 - Loss: 0.5647663474082947 - Acc: 0.9\n",
      "Iteration 1370 - Loss: 0.40860214829444885 - Acc: 1.0\n",
      "Iteration 1380 - Loss: 0.5543695688247681 - Acc: 0.9\n",
      "Iteration 1390 - Loss: 0.5334120392799377 - Acc: 0.8\n",
      "Iteration 1400 - Loss: 0.4959177076816559 - Acc: 0.9\n",
      "Iteration 1410 - Loss: 0.5440828204154968 - Acc: 0.95\n",
      "Iteration 1420 - Loss: 0.4748513102531433 - Acc: 0.95\n",
      "Iteration 1430 - Loss: 0.5590764284133911 - Acc: 0.8\n",
      "Iteration 1440 - Loss: 0.5165257453918457 - Acc: 0.9\n",
      "Iteration 1450 - Loss: 0.5410305261611938 - Acc: 0.85\n",
      "Iteration 1460 - Loss: 0.43064433336257935 - Acc: 0.95\n",
      "Iteration 1470 - Loss: 0.521945059299469 - Acc: 0.9\n",
      "Iteration 1480 - Loss: 0.5351150035858154 - Acc: 0.85\n",
      "Iteration 1490 - Loss: 0.5754531025886536 - Acc: 0.8\n",
      "Iteration 1500 - Loss: 0.5662932991981506 - Acc: 0.85\n",
      "Iteration 1510 - Loss: 0.5504074096679688 - Acc: 0.75\n",
      "Iteration 1520 - Loss: 0.4976221024990082 - Acc: 0.95\n",
      "Iteration 1530 - Loss: 0.5771495699882507 - Acc: 0.75\n",
      "Iteration 1540 - Loss: 0.5428986549377441 - Acc: 0.85\n",
      "Iteration 1550 - Loss: 0.473369300365448 - Acc: 0.9\n",
      "Iteration 1560 - Loss: 0.4938725531101227 - Acc: 0.9\n",
      "Iteration 1570 - Loss: 0.4754500389099121 - Acc: 0.9\n",
      "Iteration 1580 - Loss: 0.557696521282196 - Acc: 0.85\n",
      "Iteration 1590 - Loss: 0.4885900616645813 - Acc: 0.95\n",
      "Iteration 1600 - Loss: 0.49466627836227417 - Acc: 0.85\n",
      "Iteration 1610 - Loss: 0.5126368999481201 - Acc: 0.95\n",
      "Iteration 1620 - Loss: 0.6401354074478149 - Acc: 0.7\n",
      "Iteration 1630 - Loss: 0.5727237462997437 - Acc: 0.85\n",
      "Iteration 1640 - Loss: 0.41408759355545044 - Acc: 0.9\n",
      "Iteration 1650 - Loss: 0.503625214099884 - Acc: 0.85\n",
      "Iteration 1660 - Loss: 0.4914620816707611 - Acc: 0.95\n",
      "Iteration 1670 - Loss: 0.46559423208236694 - Acc: 1.0\n",
      "Iteration 1680 - Loss: 0.4693339765071869 - Acc: 0.9\n",
      "Iteration 1690 - Loss: 0.5279061198234558 - Acc: 0.95\n",
      "Iteration 1700 - Loss: 0.5991320610046387 - Acc: 0.85\n",
      "Iteration 1710 - Loss: 0.5744059681892395 - Acc: 0.8\n",
      "Iteration 1720 - Loss: 0.5773023366928101 - Acc: 0.75\n",
      "Iteration 1730 - Loss: 0.47931838035583496 - Acc: 0.95\n",
      "Iteration 1740 - Loss: 0.535913348197937 - Acc: 0.75\n",
      "Iteration 1750 - Loss: 0.5229995846748352 - Acc: 0.85\n",
      "Iteration 1760 - Loss: 0.4299929738044739 - Acc: 0.9\n",
      "Iteration 1770 - Loss: 0.491325318813324 - Acc: 0.95\n",
      "Iteration 1780 - Loss: 0.49932432174682617 - Acc: 0.95\n",
      "Iteration 1790 - Loss: 0.4485696852207184 - Acc: 0.9\n",
      "Iteration 1800 - Loss: 0.5810613036155701 - Acc: 0.8\n",
      "Iteration 1810 - Loss: 0.431331068277359 - Acc: 0.95\n",
      "Iteration 1820 - Loss: 0.5114260315895081 - Acc: 0.85\n",
      "Iteration 1830 - Loss: 0.7032727599143982 - Acc: 0.65\n",
      "Iteration 1840 - Loss: 0.4700535833835602 - Acc: 0.95\n",
      "Iteration 1850 - Loss: 0.49989932775497437 - Acc: 0.9\n",
      "Iteration 1860 - Loss: 0.4654799997806549 - Acc: 0.9\n",
      "Iteration 1870 - Loss: 0.5701856017112732 - Acc: 0.8\n",
      "Iteration 1880 - Loss: 0.4829922616481781 - Acc: 0.85\n",
      "Iteration 1890 - Loss: 0.41687482595443726 - Acc: 0.95\n",
      "Iteration 1900 - Loss: 0.5160394310951233 - Acc: 0.85\n",
      "Iteration 1910 - Loss: 0.6048610210418701 - Acc: 0.7\n",
      "Iteration 1920 - Loss: 0.63604736328125 - Acc: 0.8\n",
      "Iteration 1930 - Loss: 0.5546576380729675 - Acc: 0.75\n",
      "Iteration 1940 - Loss: 0.5186020135879517 - Acc: 0.85\n",
      "Iteration 1950 - Loss: 0.5484397411346436 - Acc: 0.9\n",
      "Iteration 1960 - Loss: 0.45786285400390625 - Acc: 1.0\n",
      "Iteration 1970 - Loss: 0.4355900287628174 - Acc: 0.95\n",
      "Iteration 1980 - Loss: 0.4754526615142822 - Acc: 0.95\n",
      "Iteration 1990 - Loss: 0.4671788215637207 - Acc: 0.95\n",
      "Iteration 2000 - Loss: 0.6177803874015808 - Acc: 0.85\n",
      "Iteration 2010 - Loss: 0.4830661714076996 - Acc: 0.9\n",
      "Iteration 2020 - Loss: 0.6474776864051819 - Acc: 0.75\n",
      "Iteration 2030 - Loss: 0.5600378513336182 - Acc: 0.8\n",
      "Iteration 2040 - Loss: 0.4834964871406555 - Acc: 0.9\n",
      "Iteration 2050 - Loss: 0.5462656617164612 - Acc: 0.8\n",
      "Iteration 2060 - Loss: 0.6436196565628052 - Acc: 0.75\n",
      "Iteration 2070 - Loss: 0.4494187831878662 - Acc: 0.9\n",
      "Iteration 2080 - Loss: 0.5500968098640442 - Acc: 0.85\n",
      "Iteration 2090 - Loss: 0.5379074215888977 - Acc: 0.9\n",
      "Iteration 2100 - Loss: 0.5946027040481567 - Acc: 0.8\n",
      "Iteration 2110 - Loss: 0.6105763912200928 - Acc: 0.8\n",
      "Iteration 2120 - Loss: 0.41824784874916077 - Acc: 1.0\n",
      "Iteration 2130 - Loss: 0.46000927686691284 - Acc: 0.9\n",
      "Iteration 2140 - Loss: 0.4778819680213928 - Acc: 0.9\n",
      "Iteration 2150 - Loss: 0.5284274816513062 - Acc: 0.8\n",
      "Iteration 2160 - Loss: 0.5918406248092651 - Acc: 0.8\n",
      "Iteration 2170 - Loss: 0.4831686019897461 - Acc: 0.9\n",
      "Iteration 2180 - Loss: 0.42970991134643555 - Acc: 1.0\n",
      "Iteration 2190 - Loss: 0.42852258682250977 - Acc: 0.95\n",
      "Iteration 2200 - Loss: 0.5231926441192627 - Acc: 0.8\n",
      "Iteration 2210 - Loss: 0.4656481146812439 - Acc: 0.85\n",
      "Iteration 2220 - Loss: 0.6878657341003418 - Acc: 0.6\n",
      "Iteration 2230 - Loss: 0.4297761917114258 - Acc: 1.0\n",
      "Iteration 2240 - Loss: 0.5901132822036743 - Acc: 0.85\n",
      "Iteration 2250 - Loss: 0.5794453620910645 - Acc: 0.85\n",
      "Iteration 2260 - Loss: 0.5540014505386353 - Acc: 0.75\n",
      "Iteration 2270 - Loss: 0.598812997341156 - Acc: 0.8\n",
      "Iteration 2280 - Loss: 0.5990007519721985 - Acc: 0.85\n",
      "Iteration 2290 - Loss: 0.5496101975440979 - Acc: 0.85\n",
      "Iteration 2300 - Loss: 0.5073549151420593 - Acc: 0.95\n",
      "Iteration 2310 - Loss: 0.6901860237121582 - Acc: 0.8\n",
      "Iteration 2320 - Loss: 0.3638080060482025 - Acc: 1.0\n",
      "Iteration 2330 - Loss: 0.5391401648521423 - Acc: 0.9\n",
      "Iteration 2340 - Loss: 0.4617484211921692 - Acc: 0.9\n",
      "Iteration 2350 - Loss: 0.4910435676574707 - Acc: 0.9\n",
      "Iteration 2360 - Loss: 0.3951168656349182 - Acc: 0.95\n",
      "Iteration 2370 - Loss: 0.5477431416511536 - Acc: 0.8\n",
      "Iteration 2380 - Loss: 0.40586328506469727 - Acc: 0.95\n",
      "Iteration 2390 - Loss: 0.5461484789848328 - Acc: 0.75\n",
      "Iteration 2400 - Loss: 0.44780945777893066 - Acc: 0.95\n",
      "Iteration 2410 - Loss: 0.47540849447250366 - Acc: 0.9\n",
      "Iteration 2420 - Loss: 0.4463126063346863 - Acc: 0.95\n",
      "Iteration 2430 - Loss: 0.4312695562839508 - Acc: 0.9\n",
      "Iteration 2440 - Loss: 0.520600438117981 - Acc: 0.8\n",
      "Iteration 2450 - Loss: 0.5566949844360352 - Acc: 0.8\n",
      "Iteration 2460 - Loss: 0.5790874361991882 - Acc: 0.7\n",
      "Iteration 2470 - Loss: 0.5087834000587463 - Acc: 0.8\n",
      "Iteration 2480 - Loss: 0.619472861289978 - Acc: 0.8\n",
      "Iteration 2490 - Loss: 0.4675602316856384 - Acc: 0.9\n",
      "Iteration 2500 - Loss: 0.583075761795044 - Acc: 0.7\n",
      "Iteration 2510 - Loss: 0.4977571964263916 - Acc: 0.85\n",
      "Iteration 2520 - Loss: 0.45696592330932617 - Acc: 0.9\n",
      "Iteration 2530 - Loss: 0.4804597795009613 - Acc: 0.95\n",
      "Iteration 2540 - Loss: 0.4772336483001709 - Acc: 0.9\n",
      "Iteration 2550 - Loss: 0.41653889417648315 - Acc: 0.9\n",
      "Iteration 2560 - Loss: 0.4861670434474945 - Acc: 0.9\n",
      "Iteration 2570 - Loss: 0.6754107475280762 - Acc: 0.65\n",
      "Iteration 2580 - Loss: 0.574161171913147 - Acc: 0.8\n",
      "Iteration 2590 - Loss: 0.4804786741733551 - Acc: 0.85\n",
      "Iteration 2600 - Loss: 0.5104228258132935 - Acc: 0.9\n",
      "Iteration 2610 - Loss: 0.3560025691986084 - Acc: 1.0\n",
      "Iteration 2620 - Loss: 0.5028931498527527 - Acc: 0.85\n",
      "Iteration 2630 - Loss: 0.6283190846443176 - Acc: 0.7\n",
      "Iteration 2640 - Loss: 0.515181839466095 - Acc: 0.9\n",
      "Iteration 2650 - Loss: 0.4090278744697571 - Acc: 0.9\n",
      "Iteration 2660 - Loss: 0.6307552456855774 - Acc: 0.8\n",
      "Iteration 2670 - Loss: 0.4559798240661621 - Acc: 0.95\n",
      "Iteration 2680 - Loss: 0.4128594994544983 - Acc: 0.9\n",
      "Iteration 2690 - Loss: 0.4848793148994446 - Acc: 0.9\n",
      "Iteration 2700 - Loss: 0.43515950441360474 - Acc: 0.95\n",
      "Iteration 2710 - Loss: 0.49887245893478394 - Acc: 0.9\n",
      "Iteration 2720 - Loss: 0.440115362405777 - Acc: 0.85\n",
      "Iteration 2730 - Loss: 0.47860050201416016 - Acc: 0.9\n",
      "Iteration 2740 - Loss: 0.4030526280403137 - Acc: 0.95\n",
      "Iteration 2750 - Loss: 0.48516911268234253 - Acc: 0.85\n",
      "Iteration 2760 - Loss: 0.48120301961898804 - Acc: 0.95\n",
      "Iteration 2770 - Loss: 0.5422495603561401 - Acc: 0.8\n",
      "Iteration 2780 - Loss: 0.4000425934791565 - Acc: 0.9\n",
      "Iteration 2790 - Loss: 0.4646507203578949 - Acc: 0.85\n",
      "Iteration 2800 - Loss: 0.4221046566963196 - Acc: 0.9\n",
      "Iteration 2810 - Loss: 0.5146183371543884 - Acc: 0.85\n",
      "Iteration 2820 - Loss: 0.49537190794944763 - Acc: 0.9\n",
      "Iteration 2830 - Loss: 0.5621302723884583 - Acc: 0.8\n",
      "Iteration 2840 - Loss: 0.5146505832672119 - Acc: 0.8\n",
      "Iteration 2850 - Loss: 0.6586078405380249 - Acc: 0.75\n",
      "Iteration 2860 - Loss: 0.5327854752540588 - Acc: 0.8\n",
      "Iteration 2870 - Loss: 0.631389319896698 - Acc: 0.7\n",
      "Iteration 2880 - Loss: 0.47250062227249146 - Acc: 0.85\n",
      "Iteration 2890 - Loss: 0.6283558011054993 - Acc: 0.75\n",
      "Iteration 2900 - Loss: 0.5432348251342773 - Acc: 0.95\n",
      "Iteration 2910 - Loss: 0.4068329930305481 - Acc: 0.95\n",
      "Iteration 2920 - Loss: 0.4228021502494812 - Acc: 0.95\n",
      "Iteration 2930 - Loss: 0.41971102356910706 - Acc: 0.95\n",
      "Iteration 2940 - Loss: 0.48970651626586914 - Acc: 0.85\n",
      "Iteration 2950 - Loss: 0.5762361288070679 - Acc: 0.85\n",
      "Iteration 2960 - Loss: 0.4987885355949402 - Acc: 0.85\n",
      "Iteration 2970 - Loss: 0.4528277516365051 - Acc: 0.9\n",
      "Iteration 2980 - Loss: 0.6214218139648438 - Acc: 0.75\n",
      "Iteration 2990 - Loss: 0.4308668076992035 - Acc: 0.9\n",
      "Iteration 3000 - Loss: 0.4583287835121155 - Acc: 0.9\n",
      "Iteration 3010 - Loss: 0.562538206577301 - Acc: 0.8\n",
      "Iteration 3020 - Loss: 0.5145351886749268 - Acc: 0.85\n",
      "Iteration 3030 - Loss: 0.46191897988319397 - Acc: 0.95\n",
      "Iteration 3040 - Loss: 0.43908530473709106 - Acc: 0.95\n",
      "Iteration 3050 - Loss: 0.42493289709091187 - Acc: 0.85\n",
      "Iteration 3060 - Loss: 0.5224851369857788 - Acc: 0.85\n",
      "Iteration 3070 - Loss: 0.4810008406639099 - Acc: 0.95\n",
      "Iteration 3080 - Loss: 0.3728043735027313 - Acc: 1.0\n",
      "Iteration 3090 - Loss: 0.45164284110069275 - Acc: 0.9\n",
      "Iteration 3100 - Loss: 0.5962696075439453 - Acc: 0.75\n",
      "Iteration 3110 - Loss: 0.46326079964637756 - Acc: 0.85\n",
      "Iteration 3120 - Loss: 0.4379851222038269 - Acc: 0.8\n",
      "Iteration 3130 - Loss: 0.33235085010528564 - Acc: 1.0\n",
      "Iteration 3140 - Loss: 0.5250611305236816 - Acc: 0.85\n",
      "Iteration 3150 - Loss: 0.46136799454689026 - Acc: 0.85\n",
      "Iteration 3160 - Loss: 0.4301985800266266 - Acc: 0.95\n",
      "Iteration 3170 - Loss: 0.6565840840339661 - Acc: 0.75\n",
      "Iteration 3180 - Loss: 0.35395896434783936 - Acc: 1.0\n",
      "Iteration 3190 - Loss: 0.4337044358253479 - Acc: 0.95\n",
      "Iteration 3200 - Loss: 0.6177666783332825 - Acc: 0.7\n",
      "Iteration 3210 - Loss: 0.3915959596633911 - Acc: 0.95\n",
      "Iteration 3220 - Loss: 0.46286875009536743 - Acc: 0.9\n",
      "Iteration 3230 - Loss: 0.35622429847717285 - Acc: 0.95\n",
      "Iteration 3240 - Loss: 0.43601274490356445 - Acc: 0.9\n",
      "Iteration 3250 - Loss: 0.41444581747055054 - Acc: 0.85\n",
      "Iteration 3260 - Loss: 0.4274449944496155 - Acc: 0.95\n",
      "Iteration 3270 - Loss: 0.45990410447120667 - Acc: 0.85\n",
      "Iteration 3280 - Loss: 0.48462170362472534 - Acc: 0.85\n",
      "Iteration 3290 - Loss: 0.39376723766326904 - Acc: 0.95\n",
      "Iteration 3300 - Loss: 0.4717479646205902 - Acc: 0.85\n",
      "Iteration 3310 - Loss: 0.39047062397003174 - Acc: 0.95\n",
      "Iteration 3320 - Loss: 0.33486509323120117 - Acc: 1.0\n",
      "Iteration 3330 - Loss: 0.4529041647911072 - Acc: 0.85\n",
      "Iteration 3340 - Loss: 0.4274734556674957 - Acc: 0.95\n",
      "Iteration 3350 - Loss: 0.4301794469356537 - Acc: 0.9\n",
      "Iteration 3360 - Loss: 0.4475283622741699 - Acc: 0.95\n",
      "Iteration 3370 - Loss: 0.5306742787361145 - Acc: 0.8\n",
      "Iteration 3380 - Loss: 0.48478928208351135 - Acc: 0.85\n",
      "Iteration 3390 - Loss: 0.4815393388271332 - Acc: 0.9\n",
      "Iteration 3400 - Loss: 0.6113913059234619 - Acc: 0.85\n",
      "Iteration 3410 - Loss: 0.49856019020080566 - Acc: 0.85\n",
      "Iteration 3420 - Loss: 0.3898504972457886 - Acc: 0.9\n",
      "Iteration 3430 - Loss: 0.5224171876907349 - Acc: 0.85\n",
      "Iteration 3440 - Loss: 0.35146310925483704 - Acc: 0.95\n",
      "Iteration 3450 - Loss: 0.5722379088401794 - Acc: 0.8\n",
      "Iteration 3460 - Loss: 0.43961796164512634 - Acc: 0.95\n",
      "Iteration 3470 - Loss: 0.36660677194595337 - Acc: 0.9\n",
      "Iteration 3480 - Loss: 0.4425671696662903 - Acc: 0.9\n",
      "Iteration 3490 - Loss: 0.4967513978481293 - Acc: 0.85\n",
      "Iteration 3500 - Loss: 0.5473317503929138 - Acc: 0.75\n",
      "Iteration 3510 - Loss: 0.5011301040649414 - Acc: 0.8\n",
      "Iteration 3520 - Loss: 0.361590713262558 - Acc: 0.95\n",
      "Iteration 3530 - Loss: 0.5024024248123169 - Acc: 0.8\n",
      "Iteration 3540 - Loss: 0.5001621246337891 - Acc: 0.95\n",
      "Iteration 3550 - Loss: 0.43694743514060974 - Acc: 0.95\n",
      "Iteration 3560 - Loss: 0.4844920039176941 - Acc: 0.9\n",
      "Iteration 3570 - Loss: 0.6500396728515625 - Acc: 0.75\n",
      "Iteration 3580 - Loss: 0.5163424015045166 - Acc: 0.85\n",
      "Iteration 3590 - Loss: 0.44375887513160706 - Acc: 0.95\n",
      "Iteration 3600 - Loss: 0.4055909514427185 - Acc: 0.9\n",
      "Iteration 3610 - Loss: 0.561123788356781 - Acc: 0.75\n",
      "Iteration 3620 - Loss: 0.4801956117153168 - Acc: 0.85\n",
      "Iteration 3630 - Loss: 0.5001267194747925 - Acc: 0.9\n",
      "Iteration 3640 - Loss: 0.46156007051467896 - Acc: 0.9\n",
      "Iteration 3650 - Loss: 0.4160929322242737 - Acc: 1.0\n",
      "Iteration 3660 - Loss: 0.48771533370018005 - Acc: 0.85\n",
      "Iteration 3670 - Loss: 0.4346577525138855 - Acc: 0.85\n",
      "Iteration 3680 - Loss: 0.4371415674686432 - Acc: 0.9\n",
      "Iteration 3690 - Loss: 0.5741848945617676 - Acc: 0.8\n",
      "Iteration 3700 - Loss: 0.524399995803833 - Acc: 0.85\n",
      "Iteration 3710 - Loss: 0.534837543964386 - Acc: 0.75\n",
      "Iteration 3720 - Loss: 0.41525202989578247 - Acc: 0.9\n",
      "Iteration 3730 - Loss: 0.3987041413784027 - Acc: 0.95\n",
      "Iteration 3740 - Loss: 0.4913819432258606 - Acc: 0.8\n",
      "Iteration 3750 - Loss: 0.37968751788139343 - Acc: 1.0\n",
      "Iteration 3760 - Loss: 0.49155622720718384 - Acc: 0.9\n",
      "Iteration 3770 - Loss: 0.47165146470069885 - Acc: 0.9\n",
      "Iteration 3780 - Loss: 0.5412253141403198 - Acc: 0.8\n",
      "Iteration 3790 - Loss: 0.42710739374160767 - Acc: 0.95\n",
      "Iteration 3800 - Loss: 0.4441622793674469 - Acc: 0.9\n",
      "Iteration 3810 - Loss: 0.34879162907600403 - Acc: 1.0\n",
      "Iteration 3820 - Loss: 0.5197850465774536 - Acc: 0.85\n",
      "Iteration 3830 - Loss: 0.5189448595046997 - Acc: 0.9\n",
      "Iteration 3840 - Loss: 0.3833904266357422 - Acc: 0.95\n",
      "Iteration 3850 - Loss: 0.5463732481002808 - Acc: 0.8\n",
      "Iteration 3860 - Loss: 0.3874244689941406 - Acc: 0.95\n",
      "Iteration 3870 - Loss: 0.43871551752090454 - Acc: 0.9\n",
      "Iteration 3880 - Loss: 0.49064087867736816 - Acc: 0.85\n",
      "Iteration 3890 - Loss: 0.43764153122901917 - Acc: 0.85\n",
      "Iteration 3900 - Loss: 0.43099790811538696 - Acc: 1.0\n",
      "Iteration 3910 - Loss: 0.3918890058994293 - Acc: 0.95\n",
      "Iteration 3920 - Loss: 0.6168407201766968 - Acc: 0.7\n",
      "Iteration 3930 - Loss: 0.554650068283081 - Acc: 0.75\n",
      "Iteration 3940 - Loss: 0.5055276155471802 - Acc: 0.9\n",
      "Iteration 3950 - Loss: 0.600965678691864 - Acc: 0.7\n",
      "Iteration 3960 - Loss: 0.48717015981674194 - Acc: 0.85\n",
      "Iteration 3970 - Loss: 0.5707013010978699 - Acc: 0.75\n",
      "Iteration 3980 - Loss: 0.3916108310222626 - Acc: 0.95\n",
      "Iteration 3990 - Loss: 0.3850339651107788 - Acc: 0.95\n",
      "Iteration 4000 - Loss: 0.4477313458919525 - Acc: 0.95\n",
      "Iteration 4010 - Loss: 0.4290603995323181 - Acc: 0.9\n",
      "Iteration 4020 - Loss: 0.4379204213619232 - Acc: 0.95\n",
      "Iteration 4030 - Loss: 0.5197136998176575 - Acc: 0.85\n",
      "Iteration 4040 - Loss: 0.4796033501625061 - Acc: 0.9\n",
      "Iteration 4050 - Loss: 0.42110738158226013 - Acc: 0.9\n",
      "Iteration 4060 - Loss: 0.41776609420776367 - Acc: 0.9\n",
      "Iteration 4070 - Loss: 0.4639773368835449 - Acc: 0.9\n",
      "Iteration 4080 - Loss: 0.4174579977989197 - Acc: 0.95\n",
      "Iteration 4090 - Loss: 0.46569642424583435 - Acc: 0.85\n",
      "Iteration 4100 - Loss: 0.45389121770858765 - Acc: 0.85\n",
      "Iteration 4110 - Loss: 0.5624902248382568 - Acc: 0.8\n",
      "Iteration 4120 - Loss: 0.3682841658592224 - Acc: 0.95\n",
      "Iteration 4130 - Loss: 0.42748188972473145 - Acc: 0.9\n",
      "Iteration 4140 - Loss: 0.3942544162273407 - Acc: 0.95\n",
      "Iteration 4150 - Loss: 0.5522178411483765 - Acc: 0.8\n",
      "Iteration 4160 - Loss: 0.3134954571723938 - Acc: 1.0\n",
      "Iteration 4170 - Loss: 0.5412225127220154 - Acc: 0.85\n",
      "Iteration 4180 - Loss: 0.4256022870540619 - Acc: 0.95\n",
      "Iteration 4190 - Loss: 0.5507978200912476 - Acc: 0.8\n",
      "Iteration 4200 - Loss: 0.4793168902397156 - Acc: 0.9\n",
      "Iteration 4210 - Loss: 0.6044652462005615 - Acc: 0.8\n",
      "Iteration 4220 - Loss: 0.4499802589416504 - Acc: 0.9\n",
      "Iteration 4230 - Loss: 0.38065940141677856 - Acc: 0.95\n",
      "Iteration 4240 - Loss: 0.417675644159317 - Acc: 0.9\n",
      "Iteration 4250 - Loss: 0.43494540452957153 - Acc: 0.9\n",
      "Iteration 4260 - Loss: 0.44873350858688354 - Acc: 0.9\n",
      "Iteration 4270 - Loss: 0.41705092787742615 - Acc: 0.9\n",
      "Iteration 4280 - Loss: 0.36024942994117737 - Acc: 1.0\n",
      "Iteration 4290 - Loss: 0.49593931436538696 - Acc: 0.85\n",
      "Iteration 4300 - Loss: 0.35805070400238037 - Acc: 0.95\n",
      "Iteration 4310 - Loss: 0.4997214674949646 - Acc: 0.85\n",
      "Iteration 4320 - Loss: 0.5297065377235413 - Acc: 0.85\n",
      "Iteration 4330 - Loss: 0.4532741606235504 - Acc: 0.85\n",
      "Iteration 4340 - Loss: 0.641995370388031 - Acc: 0.7\n",
      "Iteration 4350 - Loss: 0.5538647770881653 - Acc: 0.75\n",
      "Iteration 4360 - Loss: 0.5220990777015686 - Acc: 0.85\n",
      "Iteration 4370 - Loss: 0.5423507690429688 - Acc: 0.85\n",
      "Iteration 4380 - Loss: 0.6132591962814331 - Acc: 0.85\n",
      "Iteration 4390 - Loss: 0.4867796301841736 - Acc: 0.9\n",
      "Iteration 4400 - Loss: 0.4947569966316223 - Acc: 0.9\n",
      "Iteration 4410 - Loss: 0.5078603029251099 - Acc: 0.9\n",
      "Iteration 4420 - Loss: 0.3960186243057251 - Acc: 0.95\n",
      "Iteration 4430 - Loss: 0.43690791726112366 - Acc: 0.95\n",
      "Iteration 4440 - Loss: 0.4759240746498108 - Acc: 0.9\n",
      "Iteration 4450 - Loss: 0.3444938361644745 - Acc: 0.95\n",
      "Iteration 4460 - Loss: 0.3761623203754425 - Acc: 0.95\n",
      "Iteration 4470 - Loss: 0.43719157576560974 - Acc: 0.85\n",
      "Iteration 4480 - Loss: 0.35102006793022156 - Acc: 0.95\n",
      "Iteration 4490 - Loss: 0.6024337410926819 - Acc: 0.8\n",
      "Iteration 4500 - Loss: 0.49287575483322144 - Acc: 0.9\n",
      "Iteration 4510 - Loss: 0.5191664099693298 - Acc: 0.85\n",
      "Iteration 4520 - Loss: 0.3689711093902588 - Acc: 0.95\n",
      "Iteration 4530 - Loss: 0.4873313307762146 - Acc: 0.85\n",
      "Iteration 4540 - Loss: 0.5076633095741272 - Acc: 0.8\n",
      "Iteration 4550 - Loss: 0.5764082670211792 - Acc: 0.85\n",
      "Iteration 4560 - Loss: 0.4652043282985687 - Acc: 0.9\n",
      "Iteration 4570 - Loss: 0.39500492811203003 - Acc: 0.9\n",
      "Iteration 4580 - Loss: 0.37435656785964966 - Acc: 0.95\n",
      "Iteration 4590 - Loss: 0.3946831524372101 - Acc: 0.95\n",
      "Iteration 4600 - Loss: 0.37303879857063293 - Acc: 0.95\n",
      "Iteration 4610 - Loss: 0.4242824912071228 - Acc: 0.85\n",
      "Iteration 4620 - Loss: 0.4434930682182312 - Acc: 0.85\n",
      "Iteration 4630 - Loss: 0.575372576713562 - Acc: 0.8\n",
      "Iteration 4640 - Loss: 0.421415239572525 - Acc: 0.9\n",
      "Iteration 4650 - Loss: 0.5514895915985107 - Acc: 0.8\n",
      "Iteration 4660 - Loss: 0.40226298570632935 - Acc: 0.9\n",
      "Iteration 4670 - Loss: 0.3488597273826599 - Acc: 1.0\n",
      "Iteration 4680 - Loss: 0.3445526361465454 - Acc: 0.95\n",
      "Iteration 4690 - Loss: 0.4089476466178894 - Acc: 1.0\n",
      "Iteration 4700 - Loss: 0.42036810517311096 - Acc: 0.95\n",
      "Iteration 4710 - Loss: 0.4661831259727478 - Acc: 0.9\n",
      "Iteration 4720 - Loss: 0.4115370810031891 - Acc: 0.95\n",
      "Iteration 4730 - Loss: 0.3929574489593506 - Acc: 0.85\n",
      "Iteration 4740 - Loss: 0.38097432255744934 - Acc: 0.95\n",
      "Iteration 4750 - Loss: 0.514506459236145 - Acc: 0.85\n",
      "Iteration 4760 - Loss: 0.4241965413093567 - Acc: 0.95\n",
      "Iteration 4770 - Loss: 0.5822197198867798 - Acc: 0.8\n",
      "Iteration 4780 - Loss: 0.4602763056755066 - Acc: 0.9\n",
      "Iteration 4790 - Loss: 0.42445582151412964 - Acc: 0.9\n",
      "Iteration 4800 - Loss: 0.37898188829421997 - Acc: 0.95\n",
      "Iteration 4810 - Loss: 0.3232514262199402 - Acc: 1.0\n",
      "Iteration 4820 - Loss: 0.35458904504776 - Acc: 1.0\n",
      "Iteration 4830 - Loss: 0.4354614317417145 - Acc: 0.9\n",
      "Iteration 4840 - Loss: 0.45403939485549927 - Acc: 0.85\n",
      "Iteration 4850 - Loss: 0.5154297947883606 - Acc: 0.85\n",
      "Iteration 4860 - Loss: 0.40924787521362305 - Acc: 0.95\n",
      "Iteration 4870 - Loss: 0.3731374740600586 - Acc: 1.0\n",
      "Iteration 4880 - Loss: 0.40175914764404297 - Acc: 0.95\n",
      "Iteration 4890 - Loss: 0.42068105936050415 - Acc: 0.95\n",
      "Iteration 4900 - Loss: 0.3976992666721344 - Acc: 0.9\n",
      "Iteration 4910 - Loss: 0.45190197229385376 - Acc: 0.9\n",
      "Iteration 4920 - Loss: 0.500636637210846 - Acc: 0.9\n",
      "Iteration 4930 - Loss: 0.5879297256469727 - Acc: 0.85\n",
      "Iteration 4940 - Loss: 0.36745622754096985 - Acc: 0.95\n",
      "Iteration 4950 - Loss: 0.5294210910797119 - Acc: 0.8\n",
      "Iteration 4960 - Loss: 0.4422195553779602 - Acc: 0.85\n",
      "Iteration 4970 - Loss: 0.5113231539726257 - Acc: 0.85\n",
      "Iteration 4980 - Loss: 0.47050046920776367 - Acc: 0.9\n",
      "Iteration 4990 - Loss: 0.4615052342414856 - Acc: 0.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_iterations):\n",
    "    x, y = get_train_data(batch_size)\n",
    "    loss = siamese_net.train_on_batch(x, y)\n",
    "    if i % evaluateEvery == 0:\n",
    "        print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5404632882748764\n"
     ]
    }
   ],
   "source": [
    "oneshot_images_repr = encoder(oneshot_images)\n",
    "classify_images_repr = encoder(classify_images)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(oneshot_images_repr, oneshot_labels)\n",
    "\n",
    "pred = neigh.predict(classify_images_repr)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(pred == classify_labels)/len(classify_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Testing siamese network ... =======\n",
      "Class 1:  10\n",
      "Class 2:  10\n",
      "Shape:  (28, 28, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_10' (type Functional).\n    \n    Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 28, 1, 1)\n    \n    Call arguments received by layer 'model_10' (type Functional):\n      • inputs=(('tf.Tensor(shape=(None, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 28, 1), dtype=float32)'),)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Bach Khoa 3\\Git projects\\oneshot-learning\\EMNIST_siamese_network_methods.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_siamese(siamese_net, encoder, similar \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Bach Khoa 3\\Git projects\\oneshot-learning\\EMNIST_siamese_network_methods.ipynb Cell 12\u001b[0m in \u001b[0;36mtest_siamese\u001b[1;34m(siamese_net, encoder, similar)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m axes[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity: \u001b[39m\u001b[39m\"\u001b[39m, siamese_net\u001b[39m.\u001b[39;49mpredict([[img1, img2]]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m siamese_net\u001b[39m.\u001b[39mpredict([[img1, img2]])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\BACHK_~1\\AppData\\Local\\Temp\\__autograph_generated_file4932sb7j.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_10' (type Functional).\n    \n    Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 28, 1, 1)\n    \n    Call arguments received by layer 'model_10' (type Functional):\n      • inputs=(('tf.Tensor(shape=(None, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 28, 1), dtype=float32)'),)\n      • training=False\n      • mask=None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_siamese(siamese_net, encoder, similar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= CNN ae method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 32s 10ms/step - loss: 0.0212\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0130\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0118\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0111\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0107\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0104\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0102\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0100\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0099\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0098\n",
      "Vectorizing ...\n",
      "Learning oneshot ...\n",
      "Predicting ...\n",
      "Accuracy:  0.4513785505865539\n",
      "======= CNN ae method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4513785505865539"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.test_CNE(train_images, train_labels, oneshot_images, oneshot_labels, classify_images, classify_labels, 40, 32, True, 1, 28, 28, 1, 28*28)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
