{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import methods as M\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Loading emnist data ... =======\n",
      "Output shape:  [(96000, 28, 28), (96000,), (16800, 28, 28), (16800,)]\n",
      "Train labels:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 18 19 22 24 25 26 27 28\n",
      " 29 30 32 33 34 35 36 37 38 40 41 42 43 44 45 46]\n",
      "Test labels:  [14 17 20 21 23 31 39]\n",
      "======= Finished loading. =======\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, oneshot_images, oneshot_labels, classify_images, classify_labels = M.get_emnist(40, 1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_label(label):\n",
    "    return train_images[np.random.choice(np.where(train_labels == label)[0], 1, False)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(size):\n",
    "    targets = np.zeros((size,))\n",
    "    targets[size//2:] = 1\n",
    "    pairs = [np.zeros((size, 28, 28)) for _ in range(2)]\n",
    "    labels = np.unique(train_labels)\n",
    "    for i in range(size):\n",
    "        class1 = np.random.choice(labels, 1)[0]\n",
    "        class2 = class1\n",
    "        if i < size//2:\n",
    "            while class2 == class1:\n",
    "                class2 = np.random.choice(labels, 1)[0]\n",
    "        pairs[0][i] = get_image_by_label(class1)\n",
    "        pairs[1][i] = get_image_by_label(class2)\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_net_and_encoder(input_shape):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv2D(16, (3, 3), input_shape=input_shape, activation='relu', kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Conv2D(32, (3, 3), kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Flatten())\n",
    "    \n",
    "    encoder.add(Dense(32, activation='sigmoid', kernel_regularizer='l2'))\n",
    "    \n",
    "    left_emb = encoder(left_input)\n",
    "    right_emb = encoder(right_input)\n",
    "    \n",
    "    L1_Layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_emb,right_emb])\n",
    "    OP = Dense(1, activation='sigmoid', kernel_regularizer='l2')(L1_Dist)\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=OP)\n",
    "    \n",
    "    return siamese_net, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_shot_task(N):\n",
    "    labels = np.unique(train_labels)\n",
    "    cats = np.random.choice(labels, N, replace=False)\n",
    "    _, w, h = train_images.shape\n",
    "    true_cat = cats[0]\n",
    "    test_image = np.array([get_image_by_label(true_cat)]*N).reshape(N, w, h, 1)\n",
    "    support_set = get_image_by_label(cats).reshape(N, w, h, 1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image,support_set,targets = sklearn.utils.shuffle(test_image,support_set,targets)\n",
    "    \n",
    "    return [test_image,support_set], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_siamese(siamese_net, encoder, similar = None):\n",
    "    if similar == None: \n",
    "        if np.random.rand() < 1/2: return test_siamese(siamese_net, encoder, True)\n",
    "        else: return test_siamese(siamese_net, encoder, False)\n",
    "    \n",
    "    labels = np.unique(train_labels)\n",
    "    class1 = np.random.choice(labels, 1)[0]\n",
    "    class2 = class1\n",
    "    if not similar:\n",
    "        while class2 == class1:\n",
    "            class2 = np.random.choice(labels, 1)[0]\n",
    "    img1 = np.expand_dims(get_image_by_label(class1), axis=2)\n",
    "    img2 = np.expand_dims(get_image_by_label(class2), axis=2)\n",
    "    print(\"======= Testing siamese network ... =======\")\n",
    "    print(\"Class 1: \", class1)\n",
    "    print(\"Class 2: \", class2)\n",
    "    print(\"Shape: \", img1.shape)\n",
    "\n",
    "    f, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    axes[0].imshow(img1)\n",
    "    axes[1].imshow(img2)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    print(\"Similarity: \", siamese_net.predict([[img1, img2]]))\n",
    "    return siamese_net.predict([[img1, img2]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)     (None, 32)           30624       ['input_25[0][0]',               \n",
      "                                                                  'input_26[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_10 (Lambda)             (None, 32)           0           ['sequential_12[0][0]',          \n",
      "                                                                  'sequential_12[1][0]']          \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 1)            33          ['lambda_10[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,657\n",
      "Trainable params: 30,561\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 700\n",
    "batch_size = 10\n",
    "\n",
    "_, w, h = train_images.shape\n",
    "\n",
    "siamese_net, encoder = get_siamese_net_and_encoder((w, h, 1))\n",
    "\n",
    "siamese_net.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, num_iterations):\n",
    "    x, y = get_train_data(batch_size)\n",
    "    siamese_net.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.45691657238134936\n"
     ]
    }
   ],
   "source": [
    "oneshot_images_repr = encoder(oneshot_images)\n",
    "classify_images_repr = encoder(classify_images)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(oneshot_images_repr, oneshot_labels)\n",
    "\n",
    "pred = neigh.predict(classify_images_repr)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(pred == classify_labels)/len(classify_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Testing siamese network ... =======\n",
      "Class 1:  33\n",
      "Class 2:  33\n",
      "Shape:  (28, 28, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_10' (type Functional).\n    \n    Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 28, 1, 1)\n    \n    Call arguments received by layer 'model_10' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 28, 1), dtype=float32)')\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Bach Khoa 3\\Git projects\\oneshot-learning\\EMNIST_siamese_network_methods.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_siamese(siamese_net, encoder, similar \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Bach Khoa 3\\Git projects\\oneshot-learning\\EMNIST_siamese_network_methods.ipynb Cell 11\u001b[0m in \u001b[0;36mtest_siamese\u001b[1;34m(siamese_net, encoder, similar)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m axes[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity: \u001b[39m\u001b[39m\"\u001b[39m, siamese_net\u001b[39m.\u001b[39;49mpredict([img1, img2]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Bach%20Khoa%203/Git%20projects/oneshot-learning/EMNIST_siamese_network_methods.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m siamese_net\u001b[39m.\u001b[39mpredict([img1, img2])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\BACHK_~1\\AppData\\Local\\Temp\\__autograph_generated_file4932sb7j.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bachk_gitmnel\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_10' (type Functional).\n    \n    Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 28, 1, 1)\n    \n    Call arguments received by layer 'model_10' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 28, 1), dtype=float32)')\n      • training=False\n      • mask=None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_siamese(siamese_net, encoder, similar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= CNN ae method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 32s 10ms/step - loss: 0.0212\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0130\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0118\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0111\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0107\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0104\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0102\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0100\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 29s 10ms/step - loss: 0.0099\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 30s 10ms/step - loss: 0.0098\n",
      "Vectorizing ...\n",
      "Learning oneshot ...\n",
      "Predicting ...\n",
      "Accuracy:  0.4513785505865539\n",
      "======= CNN ae method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4513785505865539"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.test_CNE(train_images, train_labels, oneshot_images, oneshot_labels, classify_images, classify_labels, 40, 32, True, 1, 28, 28, 1, 28*28)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
