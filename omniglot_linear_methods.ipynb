{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from emnist) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from emnist) (1.23.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from emnist) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->emnist) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->emnist) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->emnist) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->emnist) (3.4)\n",
      "Installing collected packages: emnist\n",
      "Successfully installed emnist-0.0\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (1.23.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.2)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (4.64.1)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (2.2.0)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (4.21.12)\n",
      "Collecting array-record\n",
      "  Downloading array_record-0.2.0-py310-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow_datasets) (8.1.3)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.10.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.11)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=e40338779b7ff15c76cbedeedbd63c00622bc1f0da4a473e9b57ee36d05664bd\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/76/40/54/417a4d64a01b61b247658d83597e1dc83c3de01fc0cef44972\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, toml, promise, googleapis-common-protos, etils, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "Successfully installed array-record-0.2.0 dm-tree-0.1.8 etils-1.3.0 googleapis-common-protos-1.59.0 promise-2.3 tensorflow-metadata-1.13.1 tensorflow_datasets-4.9.2 toml-0.10.2\n",
      "Collecting opencv_python\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from opencv_python) (1.23.3)\n",
      "Installing collected packages: opencv_python\n",
      "Successfully installed opencv_python-4.7.0.72\n"
     ]
    }
   ],
   "source": [
    "#!pip show tensorflow_datasets\n",
    "!pip install emnist\n",
    "!pip install tensorflow_datasets\n",
    "!pip install opencv_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), info = tfds.load('omniglot', split=['small1', 'test'], with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tfds.as_dataframe(ds_train, info)\n",
    "df_test  = tfds.as_dataframe(ds_test, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2720, 105, 105, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.stack(df_train['image'])\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alphabet', 'alphabet_char_id', 'image', 'label'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[['alphabet_char_id', 'label']].loc[np.where((df_train['alphabet'] == 27) & (df_train['alphabet_char_id'] == 23))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data handling general functions '''\n",
    "\n",
    "def separate_fewshot(test_images, test_labels, n=1):\n",
    "    oneshot_data = []\n",
    "    classify_data = []\n",
    "    for label in np.unique(test_labels):\n",
    "        for num in np.random.choice(np.where(test_labels == label)[0], n, False):\n",
    "            oneshot_data.append(num)\n",
    "    temp = set(oneshot_data)\n",
    "    for i in range(len(test_labels)):\n",
    "        if not i in temp: classify_data.append(i)\n",
    "    oneshot_images = test_images[oneshot_data]\n",
    "    oneshot_labels = test_labels[oneshot_data]\n",
    "    classify_images = test_images[classify_data]\n",
    "    classify_labels = test_labels[classify_data]\n",
    "    return oneshot_images, oneshot_labels, classify_images, classify_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, size):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        resized_image = cv2.resize(img, (56, 56))\n",
    "        resized_images.append(resized_image)\n",
    "    return np.array(resized_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_omniglot_dataframe(df, img_size = 56):\n",
    "    images = resize_images(df['image'], img_size)\n",
    "    images = images.reshape(-1, img_size * img_size * 3)\n",
    "    labels = df['label'].to_numpy()\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13180, 9408)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lbl = parse_omniglot_dataframe(df_test)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PCA(df_train, df_test, n=1, n_components = 32, verbose=False, train=1):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    if verbose: print(\"======= PCA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X=train_images)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = pca.transform(os_img)\n",
    "        clas_img = pca.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= PCA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= PCA method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Vectorizing ...\n",
      "Accuracy:  0.16500279530388948\n",
      "======= PCA method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16500279530388948"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_PCA(df_train, df_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LDA(df_train, df_test, n=1, n_components = 32, verbose=False, train=1, c=3):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "    \n",
    "    # unique_labels = df_train['label'].unique()\n",
    "    # subsample_index = []\n",
    "    # for label in unique_labels:\n",
    "    #     for ind in np.random.choice(np.where(df_train['label'] == label)[0], c, False):\n",
    "    #         subsample_index.append(ind)\n",
    "    # subsample_index = np.array(subsample_index)\n",
    "    # train_images = train_images[subsample_index]\n",
    "    # train_labels = train_labels[subsample_index]\n",
    "    \n",
    "    if verbose: print(\"======= LDA method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    lda = LDA(n_components=n_components)\n",
    "    lda.fit(X=train_images,y=train_labels)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        \n",
    "        os_img = lda.transform(os_img)\n",
    "        clas_img = lda.transform(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= LDA method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= LDA method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Vectorizing ...\n",
      "Accuracy:  0.04616244708889066\n",
      "======= LDA method: Finished =======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04616244708889066"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_LDA(df_train, df_test, verbose=True,c=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size//4, activation='ReLU'),\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=(input_size,))\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def linear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    \"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(code_size),\n",
    "    ])\n",
    "    \n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(input_size),\n",
    "    ])\n",
    "    \n",
    "    inputs = keras.Input(shape=(input_size,))\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    autoencoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    autoencoder.compile(optimizer='Adam', loss='MSE')\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder(df_train, df_test, autoencoder, img_size=56, num_components=32, n=1, verbose=False, train=1):\n",
    "    \n",
    "    (train_images, train_labels) = parse_omniglot_dataframe(df_train, img_size)\n",
    "    (test_images, test_labels) = parse_omniglot_dataframe(df_test, img_size)\n",
    "    t_alphabets = df_test['alphabet'].to_numpy()\n",
    "        \n",
    "    if verbose: print(\"======= NL Autoencoder method: Training and evaluating ... =======\")\n",
    "    if verbose: print(\"Learning background ...\")\n",
    "    autoencoder, encoder = autoencoder(img_size * img_size * 3, code_size=num_components)\n",
    "    autoencoder.fit(x=train_images,y=train_images, epochs=50, batch_size=64)\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    if verbose: print(\"Vectorizing ...\")\n",
    "    for alphabet in np.unique(t_alphabets):\n",
    "        ind_alphabet = np.where(t_alphabets == alphabet)[0]\n",
    "        labels = test_labels[ind_alphabet]\n",
    "        images = test_images[ind_alphabet]\n",
    "        os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=n)\n",
    "        \n",
    "        os_img = encoder.predict(os_img)\n",
    "        clas_img = encoder.predict(clas_img)\n",
    "\n",
    "        #if verbose: print(\"Learning oneshot ...\")\n",
    "        nn = min(train, 5)\n",
    "        neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "        neigh.fit(os_img, os_label)\n",
    "\n",
    "        #if verbose: print(\"Predicting ...\")\n",
    "        pred = neigh.predict(clas_img)\n",
    "        \n",
    "        matches += np.sum(pred == clas_label)\n",
    "        total += len(clas_label)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Accuracy: \", matches/total)\n",
    "        print(\"======= NL Autoencoder method: Finished =======\")\n",
    "\n",
    "    return matches/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= NL Autoencoder method: Training and evaluating ... =======\n",
      "Learning background ...\n",
      "Epoch 1/50\n",
      "16/43 [==========>...................] - ETA: 5s - loss: 60162.9414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnonlinear_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 10\u001b[0m, in \u001b[0;36mtest_autoencoder\u001b[0;34m(df_train, df_test, autoencoder, img_size, num_components, n, verbose, train)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning background ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m autoencoder, encoder \u001b[38;5;241m=\u001b[39m autoencoder(img_size \u001b[38;5;241m*\u001b[39m img_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, code_size\u001b[38;5;241m=\u001b[39mnum_components)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_autoencoder(df_train, df_test, autoencoder=nonlinear_autoencoder, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_net_and_encoder(input_shape, code_size = 0):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv2D(16, (3, 3), input_shape=input_shape, activation='relu', kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    # encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Conv2D(32, (3, 3), kernel_regularizer='l2'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    encoder.add(MaxPooling2D(pool_size=2, strides=(2, 2)))\n",
    "    # encoder.add(Dropout(0.25))\n",
    "    \n",
    "    encoder.add(Flatten())\n",
    "    \n",
    "    encoder.add(Dense(32, activation='sigmoid', kernel_regularizer='l2'))\n",
    "    \n",
    "    left_emb = encoder(left_input)\n",
    "    right_emb = encoder(right_input)\n",
    "    \n",
    "    L1_Layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_emb,right_emb])\n",
    "    OP = Dense(1, activation='sigmoid', kernel_regularizer='l2')(L1_Dist)\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=OP)\n",
    "    \n",
    "    return siamese_net, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 56, 56, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 56, 56, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_16 (Sequential)     (None, 32)           152768      ['input_30[0][0]',               \n",
      "                                                                  'input_31[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (None, 32)           0           ['sequential_16[0][0]',          \n",
      "                                                                  'sequential_16[1][0]']          \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 1)            33          ['lambda_14[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 152,801\n",
      "Trainable params: 152,705\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5000\n",
    "batch_size = 64\n",
    "evaluateEvery = 10\n",
    "\n",
    "(train_images, train_labels) = resize_images(df_train['image'], 56), df_train['label'].to_numpy()\n",
    "t_alphabets = df_test['alphabet'].to_numpy()\n",
    "\n",
    "_, w, h, c = train_images.shape\n",
    "\n",
    "siamese_net, encoder = get_siamese_net_and_encoder((w, h, c))\n",
    "\n",
    "siamese_net.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_label(label, train_images, train_labels):\n",
    "    return train_images[np.random.choice(np.where(train_labels == label)[0], 1, False)[0]]\n",
    "\n",
    "def get_train_data(size, train_images, train_labels, img_size):\n",
    "    targets = np.zeros((size,))\n",
    "    targets[size // 2:] = 1\n",
    "    pairs = [np.zeros((size, img_size, img_size, 3)) for _ in range(2)]\n",
    "    labels = np.unique(train_labels)\n",
    "    for i in range(size):\n",
    "        class1 = np.random.choice(labels, 1)[0]\n",
    "        class2 = class1\n",
    "        if i < size // 2:\n",
    "            while class2 == class1:\n",
    "                class2 = np.random.choice(labels, 1)[0]\n",
    "        pairs[0][i] = get_image_by_label(class1, train_images, train_labels)\n",
    "        pairs[1][i] = get_image_by_label(class2, train_images, train_labels)\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 - Loss: 1.5072311162948608 - Acc: 0.55\n",
      "Iteration 20 - Loss: 1.3206431865692139 - Acc: 0.64\n",
      "Iteration 30 - Loss: 1.2171999216079712 - Acc: 0.64\n",
      "Iteration 40 - Loss: 1.1695829629898071 - Acc: 0.62\n",
      "Iteration 50 - Loss: 1.1290456056594849 - Acc: 0.62\n",
      "Iteration 60 - Loss: 0.9871949553489685 - Acc: 0.77\n",
      "Iteration 70 - Loss: 0.9273760914802551 - Acc: 0.73\n",
      "Iteration 80 - Loss: 0.9421278834342957 - Acc: 0.69\n",
      "Iteration 90 - Loss: 0.9149353504180908 - Acc: 0.67\n",
      "Iteration 100 - Loss: 0.8752526044845581 - Acc: 0.66\n",
      "Iteration 110 - Loss: 0.8666126728057861 - Acc: 0.73\n",
      "Iteration 120 - Loss: 0.7953538298606873 - Acc: 0.72\n",
      "Iteration 130 - Loss: 0.8023759722709656 - Acc: 0.75\n",
      "Iteration 140 - Loss: 0.7387526035308838 - Acc: 0.78\n",
      "Iteration 150 - Loss: 0.7180373072624207 - Acc: 0.78\n",
      "Iteration 160 - Loss: 0.7810739874839783 - Acc: 0.69\n",
      "Iteration 170 - Loss: 0.6877657175064087 - Acc: 0.77\n",
      "Iteration 180 - Loss: 0.6942127346992493 - Acc: 0.78\n",
      "Iteration 190 - Loss: 0.697600781917572 - Acc: 0.86\n",
      "Iteration 200 - Loss: 0.6695175170898438 - Acc: 0.78\n",
      "Iteration 210 - Loss: 0.7496874928474426 - Acc: 0.77\n",
      "Iteration 220 - Loss: 0.6770010590553284 - Acc: 0.8\n",
      "Iteration 230 - Loss: 0.6521142721176147 - Acc: 0.81\n",
      "Iteration 240 - Loss: 0.5908403992652893 - Acc: 0.89\n",
      "Iteration 250 - Loss: 0.7180776000022888 - Acc: 0.69\n",
      "Iteration 260 - Loss: 0.6830494403839111 - Acc: 0.73\n",
      "Iteration 270 - Loss: 0.590825617313385 - Acc: 0.88\n",
      "Iteration 280 - Loss: 0.6342886686325073 - Acc: 0.84\n",
      "Iteration 290 - Loss: 0.6136620044708252 - Acc: 0.86\n",
      "Iteration 300 - Loss: 0.5706858038902283 - Acc: 0.81\n",
      "Iteration 310 - Loss: 0.6206327080726624 - Acc: 0.77\n",
      "Iteration 320 - Loss: 0.5614807605743408 - Acc: 0.89\n",
      "Iteration 330 - Loss: 0.6022972464561462 - Acc: 0.89\n",
      "Iteration 340 - Loss: 0.5974563360214233 - Acc: 0.83\n",
      "Iteration 350 - Loss: 0.6777015924453735 - Acc: 0.8\n",
      "Iteration 360 - Loss: 0.5823679566383362 - Acc: 0.83\n",
      "Iteration 370 - Loss: 0.5852863788604736 - Acc: 0.91\n",
      "Iteration 380 - Loss: 0.5992476940155029 - Acc: 0.83\n",
      "Iteration 390 - Loss: 0.6397210359573364 - Acc: 0.8\n",
      "Iteration 400 - Loss: 0.5903467535972595 - Acc: 0.83\n",
      "Iteration 410 - Loss: 0.6498231887817383 - Acc: 0.73\n",
      "Iteration 420 - Loss: 0.5892696976661682 - Acc: 0.83\n",
      "Iteration 430 - Loss: 0.5717039704322815 - Acc: 0.86\n",
      "Iteration 440 - Loss: 0.6678761839866638 - Acc: 0.8\n",
      "Iteration 450 - Loss: 0.5399786233901978 - Acc: 0.91\n",
      "Iteration 460 - Loss: 0.6309865713119507 - Acc: 0.78\n",
      "Iteration 470 - Loss: 0.6189042329788208 - Acc: 0.8\n",
      "Iteration 480 - Loss: 0.5559356212615967 - Acc: 0.92\n",
      "Iteration 490 - Loss: 0.6261454224586487 - Acc: 0.75\n",
      "Iteration 500 - Loss: 0.601086437702179 - Acc: 0.8\n",
      "Iteration 510 - Loss: 0.5904074907302856 - Acc: 0.83\n",
      "Iteration 520 - Loss: 0.5512158870697021 - Acc: 0.83\n",
      "Iteration 530 - Loss: 0.5498248934745789 - Acc: 0.88\n",
      "Iteration 540 - Loss: 0.6208968162536621 - Acc: 0.77\n",
      "Iteration 550 - Loss: 0.5304631590843201 - Acc: 0.89\n",
      "Iteration 560 - Loss: 0.5069075226783752 - Acc: 0.95\n",
      "Iteration 570 - Loss: 0.5303513407707214 - Acc: 0.88\n",
      "Iteration 580 - Loss: 0.5651737451553345 - Acc: 0.84\n",
      "Iteration 590 - Loss: 0.5451267957687378 - Acc: 0.91\n",
      "Iteration 600 - Loss: 0.5343770980834961 - Acc: 0.88\n",
      "Iteration 610 - Loss: 0.5204122066497803 - Acc: 0.92\n",
      "Iteration 620 - Loss: 0.5279397964477539 - Acc: 0.89\n",
      "Iteration 630 - Loss: 0.5733326077461243 - Acc: 0.81\n",
      "Iteration 640 - Loss: 0.5377883911132812 - Acc: 0.91\n",
      "Iteration 650 - Loss: 0.641142725944519 - Acc: 0.83\n",
      "Iteration 660 - Loss: 0.5157071352005005 - Acc: 0.89\n",
      "Iteration 670 - Loss: 0.48938918113708496 - Acc: 0.95\n",
      "Iteration 680 - Loss: 0.5679826736450195 - Acc: 0.84\n",
      "Iteration 690 - Loss: 0.6366208791732788 - Acc: 0.75\n",
      "Iteration 700 - Loss: 0.5433770418167114 - Acc: 0.86\n",
      "Iteration 710 - Loss: 0.5655099153518677 - Acc: 0.8\n",
      "Iteration 720 - Loss: 0.6488485336303711 - Acc: 0.8\n",
      "Iteration 730 - Loss: 0.5497949123382568 - Acc: 0.88\n",
      "Iteration 740 - Loss: 0.5249825119972229 - Acc: 0.89\n",
      "Iteration 750 - Loss: 0.5173197984695435 - Acc: 0.89\n",
      "Iteration 760 - Loss: 0.5539571046829224 - Acc: 0.86\n",
      "Iteration 770 - Loss: 0.48155587911605835 - Acc: 0.92\n",
      "Iteration 780 - Loss: 0.5622053146362305 - Acc: 0.88\n",
      "Iteration 790 - Loss: 0.4722731411457062 - Acc: 0.92\n",
      "Iteration 800 - Loss: 0.5210285782814026 - Acc: 0.89\n",
      "Iteration 810 - Loss: 0.4802905321121216 - Acc: 0.92\n",
      "Iteration 820 - Loss: 0.5030989050865173 - Acc: 0.88\n",
      "Iteration 830 - Loss: 0.5354458093643188 - Acc: 0.89\n",
      "Iteration 840 - Loss: 0.5127640962600708 - Acc: 0.89\n",
      "Iteration 850 - Loss: 0.6035568714141846 - Acc: 0.77\n",
      "Iteration 860 - Loss: 0.4779356122016907 - Acc: 0.92\n",
      "Iteration 870 - Loss: 0.4945589601993561 - Acc: 0.91\n",
      "Iteration 880 - Loss: 0.4964165687561035 - Acc: 0.89\n",
      "Iteration 890 - Loss: 0.5462265610694885 - Acc: 0.88\n",
      "Iteration 900 - Loss: 0.533495306968689 - Acc: 0.84\n",
      "Iteration 910 - Loss: 0.5836758613586426 - Acc: 0.86\n",
      "Iteration 920 - Loss: 0.5041444897651672 - Acc: 0.86\n",
      "Iteration 930 - Loss: 0.5460321307182312 - Acc: 0.86\n",
      "Iteration 940 - Loss: 0.5312979221343994 - Acc: 0.86\n",
      "Iteration 950 - Loss: 0.4927016496658325 - Acc: 0.95\n",
      "Iteration 960 - Loss: 0.5276055335998535 - Acc: 0.86\n",
      "Iteration 970 - Loss: 0.4574830234050751 - Acc: 0.94\n",
      "Iteration 980 - Loss: 0.4845563471317291 - Acc: 0.94\n",
      "Iteration 990 - Loss: 0.4878104627132416 - Acc: 0.91\n",
      "Iteration 1000 - Loss: 0.5157925486564636 - Acc: 0.89\n",
      "Iteration 1010 - Loss: 0.48918235301971436 - Acc: 0.89\n",
      "Iteration 1020 - Loss: 0.5950413346290588 - Acc: 0.78\n",
      "Iteration 1030 - Loss: 0.5714770555496216 - Acc: 0.84\n",
      "Iteration 1040 - Loss: 0.48753949999809265 - Acc: 0.94\n",
      "Iteration 1050 - Loss: 0.536724328994751 - Acc: 0.86\n",
      "Iteration 1060 - Loss: 0.4725008010864258 - Acc: 0.89\n",
      "Iteration 1070 - Loss: 0.4951423108577728 - Acc: 0.91\n",
      "Iteration 1080 - Loss: 0.5159084796905518 - Acc: 0.89\n",
      "Iteration 1090 - Loss: 0.48940032720565796 - Acc: 0.89\n",
      "Iteration 1100 - Loss: 0.5343703031539917 - Acc: 0.84\n",
      "Iteration 1110 - Loss: 0.534709095954895 - Acc: 0.86\n",
      "Iteration 1120 - Loss: 0.5520985722541809 - Acc: 0.86\n",
      "Iteration 1130 - Loss: 0.485166072845459 - Acc: 0.94\n",
      "Iteration 1140 - Loss: 0.5721462965011597 - Acc: 0.86\n",
      "Iteration 1150 - Loss: 0.5378304123878479 - Acc: 0.89\n",
      "Iteration 1160 - Loss: 0.48022687435150146 - Acc: 0.95\n",
      "Iteration 1170 - Loss: 0.5776182413101196 - Acc: 0.83\n",
      "Iteration 1180 - Loss: 0.4709521234035492 - Acc: 0.91\n",
      "Iteration 1190 - Loss: 0.44488799571990967 - Acc: 0.95\n",
      "Iteration 1200 - Loss: 0.49742940068244934 - Acc: 0.92\n",
      "Iteration 1210 - Loss: 0.4742542803287506 - Acc: 0.91\n",
      "Iteration 1220 - Loss: 0.5216696858406067 - Acc: 0.91\n",
      "Iteration 1230 - Loss: 0.477387934923172 - Acc: 0.88\n",
      "Iteration 1240 - Loss: 0.4711017608642578 - Acc: 0.89\n",
      "Iteration 1250 - Loss: 0.5376386046409607 - Acc: 0.88\n",
      "Iteration 1260 - Loss: 0.6056811809539795 - Acc: 0.8\n",
      "Iteration 1270 - Loss: 0.46184825897216797 - Acc: 0.92\n",
      "Iteration 1280 - Loss: 0.5406710505485535 - Acc: 0.89\n",
      "Iteration 1290 - Loss: 0.5138877034187317 - Acc: 0.91\n",
      "Iteration 1300 - Loss: 0.527073085308075 - Acc: 0.86\n",
      "Iteration 1310 - Loss: 0.47533127665519714 - Acc: 0.91\n",
      "Iteration 1320 - Loss: 0.4854789674282074 - Acc: 0.91\n",
      "Iteration 1330 - Loss: 0.49943608045578003 - Acc: 0.89\n",
      "Iteration 1340 - Loss: 0.5217500925064087 - Acc: 0.86\n",
      "Iteration 1350 - Loss: 0.502677321434021 - Acc: 0.92\n",
      "Iteration 1360 - Loss: 0.5834046006202698 - Acc: 0.83\n",
      "Iteration 1370 - Loss: 0.508449912071228 - Acc: 0.92\n",
      "Iteration 1380 - Loss: 0.5237149000167847 - Acc: 0.86\n",
      "Iteration 1390 - Loss: 0.46838101744651794 - Acc: 0.95\n",
      "Iteration 1400 - Loss: 0.4824138581752777 - Acc: 0.92\n",
      "Iteration 1410 - Loss: 0.5309363603591919 - Acc: 0.89\n",
      "Iteration 1420 - Loss: 0.47781816124916077 - Acc: 0.94\n",
      "Iteration 1430 - Loss: 0.48007526993751526 - Acc: 0.89\n",
      "Iteration 1440 - Loss: 0.4324007034301758 - Acc: 0.92\n",
      "Iteration 1450 - Loss: 0.5070692896842957 - Acc: 0.89\n",
      "Iteration 1460 - Loss: 0.4728126525878906 - Acc: 0.91\n",
      "Iteration 1470 - Loss: 0.49815526604652405 - Acc: 0.89\n",
      "Iteration 1480 - Loss: 0.5469611883163452 - Acc: 0.86\n",
      "Iteration 1490 - Loss: 0.5120790600776672 - Acc: 0.91\n",
      "Iteration 1500 - Loss: 0.47633278369903564 - Acc: 0.92\n",
      "Iteration 1510 - Loss: 0.4388795793056488 - Acc: 0.95\n",
      "Iteration 1520 - Loss: 0.4752609133720398 - Acc: 0.94\n",
      "Iteration 1530 - Loss: 0.5278558731079102 - Acc: 0.83\n",
      "Iteration 1540 - Loss: 0.4917105436325073 - Acc: 0.91\n",
      "Iteration 1550 - Loss: 0.4836229085922241 - Acc: 0.91\n",
      "Iteration 1560 - Loss: 0.46178311109542847 - Acc: 0.94\n",
      "Iteration 1570 - Loss: 0.5250317454338074 - Acc: 0.88\n",
      "Iteration 1580 - Loss: 0.5658708214759827 - Acc: 0.86\n",
      "Iteration 1590 - Loss: 0.5302199125289917 - Acc: 0.89\n",
      "Iteration 1600 - Loss: 0.4349541664123535 - Acc: 0.95\n",
      "Iteration 1610 - Loss: 0.5243042707443237 - Acc: 0.91\n",
      "Iteration 1620 - Loss: 0.5575176477432251 - Acc: 0.78\n",
      "Iteration 1630 - Loss: 0.462358295917511 - Acc: 0.92\n",
      "Iteration 1640 - Loss: 0.5198001265525818 - Acc: 0.89\n",
      "Iteration 1650 - Loss: 0.4604344964027405 - Acc: 0.95\n",
      "Iteration 1660 - Loss: 0.5438516139984131 - Acc: 0.84\n",
      "Iteration 1670 - Loss: 0.47562363743782043 - Acc: 0.89\n",
      "Iteration 1680 - Loss: 0.4652259945869446 - Acc: 0.89\n",
      "Iteration 1690 - Loss: 0.5020779967308044 - Acc: 0.88\n",
      "Iteration 1700 - Loss: 0.5007356405258179 - Acc: 0.89\n",
      "Iteration 1710 - Loss: 0.4857712686061859 - Acc: 0.92\n",
      "Iteration 1720 - Loss: 0.4741078019142151 - Acc: 0.92\n",
      "Iteration 1730 - Loss: 0.5625686049461365 - Acc: 0.89\n",
      "Iteration 1740 - Loss: 0.4600040316581726 - Acc: 0.94\n",
      "Iteration 1750 - Loss: 0.3928437829017639 - Acc: 0.98\n",
      "Iteration 1760 - Loss: 0.5098015666007996 - Acc: 0.86\n",
      "Iteration 1770 - Loss: 0.48389869928359985 - Acc: 0.89\n",
      "Iteration 1780 - Loss: 0.5053271055221558 - Acc: 0.88\n",
      "Iteration 1790 - Loss: 0.4288996756076813 - Acc: 0.91\n",
      "Iteration 1800 - Loss: 0.40006527304649353 - Acc: 0.95\n",
      "Iteration 1810 - Loss: 0.502822756767273 - Acc: 0.86\n",
      "Iteration 1820 - Loss: 0.5003189444541931 - Acc: 0.88\n",
      "Iteration 1830 - Loss: 0.5762072205543518 - Acc: 0.83\n",
      "Iteration 1840 - Loss: 0.4393079876899719 - Acc: 0.92\n",
      "Iteration 1850 - Loss: 0.47256654500961304 - Acc: 0.94\n",
      "Iteration 1860 - Loss: 0.4433962404727936 - Acc: 0.94\n",
      "Iteration 1870 - Loss: 0.4066804349422455 - Acc: 0.95\n",
      "Iteration 1880 - Loss: 0.44109663367271423 - Acc: 0.94\n",
      "Iteration 1890 - Loss: 0.4708639979362488 - Acc: 0.92\n",
      "Iteration 1900 - Loss: 0.47950872778892517 - Acc: 0.91\n",
      "Iteration 1910 - Loss: 0.4875560402870178 - Acc: 0.83\n",
      "Iteration 1920 - Loss: 0.4798928201198578 - Acc: 0.92\n",
      "Iteration 1930 - Loss: 0.46775737404823303 - Acc: 0.91\n",
      "Iteration 1940 - Loss: 0.4516178071498871 - Acc: 0.92\n",
      "Iteration 1950 - Loss: 0.4487835764884949 - Acc: 0.94\n",
      "Iteration 1960 - Loss: 0.46309694647789 - Acc: 0.91\n",
      "Iteration 1970 - Loss: 0.4273535907268524 - Acc: 0.92\n",
      "Iteration 1980 - Loss: 0.43970587849617004 - Acc: 0.95\n",
      "Iteration 1990 - Loss: 0.38138529658317566 - Acc: 0.97\n",
      "Iteration 2000 - Loss: 0.4549594819545746 - Acc: 0.94\n",
      "Iteration 2010 - Loss: 0.41082143783569336 - Acc: 0.95\n",
      "Iteration 2020 - Loss: 0.46082374453544617 - Acc: 0.89\n",
      "Iteration 2030 - Loss: 0.47209176421165466 - Acc: 0.92\n",
      "Iteration 2040 - Loss: 0.4178188741207123 - Acc: 0.97\n",
      "Iteration 2050 - Loss: 0.4392717182636261 - Acc: 0.94\n",
      "Iteration 2060 - Loss: 0.46838831901550293 - Acc: 0.94\n",
      "Iteration 2070 - Loss: 0.46355271339416504 - Acc: 0.89\n",
      "Iteration 2080 - Loss: 0.5246583819389343 - Acc: 0.84\n",
      "Iteration 2090 - Loss: 0.4181307256221771 - Acc: 0.94\n",
      "Iteration 2100 - Loss: 0.404542475938797 - Acc: 0.94\n",
      "Iteration 2110 - Loss: 0.46379250288009644 - Acc: 0.91\n",
      "Iteration 2120 - Loss: 0.462984561920166 - Acc: 0.91\n",
      "Iteration 2130 - Loss: 0.420509934425354 - Acc: 0.94\n",
      "Iteration 2140 - Loss: 0.3936956524848938 - Acc: 0.97\n",
      "Iteration 2150 - Loss: 0.565128743648529 - Acc: 0.84\n",
      "Iteration 2160 - Loss: 0.5020014643669128 - Acc: 0.88\n",
      "Iteration 2170 - Loss: 0.4650782346725464 - Acc: 0.92\n",
      "Iteration 2180 - Loss: 0.43673086166381836 - Acc: 0.94\n",
      "Iteration 2190 - Loss: 0.45622462034225464 - Acc: 0.89\n",
      "Iteration 2200 - Loss: 0.41817325353622437 - Acc: 0.92\n",
      "Iteration 2210 - Loss: 0.5175975561141968 - Acc: 0.86\n",
      "Iteration 2220 - Loss: 0.5221765637397766 - Acc: 0.86\n",
      "Iteration 2230 - Loss: 0.485564261674881 - Acc: 0.89\n",
      "Iteration 2240 - Loss: 0.5456222891807556 - Acc: 0.8\n",
      "Iteration 2250 - Loss: 0.39514705538749695 - Acc: 0.98\n",
      "Iteration 2260 - Loss: 0.48949745297431946 - Acc: 0.89\n",
      "Iteration 2270 - Loss: 0.4726240932941437 - Acc: 0.88\n",
      "Iteration 2280 - Loss: 0.46831563115119934 - Acc: 0.89\n",
      "Iteration 2290 - Loss: 0.40068238973617554 - Acc: 0.95\n",
      "Iteration 2300 - Loss: 0.39154863357543945 - Acc: 0.97\n",
      "Iteration 2310 - Loss: 0.4102388322353363 - Acc: 0.98\n",
      "Iteration 2320 - Loss: 0.5067209601402283 - Acc: 0.86\n",
      "Iteration 2330 - Loss: 0.4712678790092468 - Acc: 0.92\n",
      "Iteration 2340 - Loss: 0.4381530284881592 - Acc: 0.92\n",
      "Iteration 2350 - Loss: 0.500343918800354 - Acc: 0.88\n",
      "Iteration 2360 - Loss: 0.4922471344470978 - Acc: 0.88\n",
      "Iteration 2370 - Loss: 0.41705626249313354 - Acc: 0.97\n",
      "Iteration 2380 - Loss: 0.42460665106773376 - Acc: 0.91\n",
      "Iteration 2390 - Loss: 0.4355044960975647 - Acc: 0.91\n",
      "Iteration 2400 - Loss: 0.41043373942375183 - Acc: 0.95\n",
      "Iteration 2410 - Loss: 0.450114369392395 - Acc: 0.94\n",
      "Iteration 2420 - Loss: 0.3904205858707428 - Acc: 0.95\n",
      "Iteration 2430 - Loss: 0.44525110721588135 - Acc: 0.92\n",
      "Iteration 2440 - Loss: 0.43767309188842773 - Acc: 0.94\n",
      "Iteration 2450 - Loss: 0.504432201385498 - Acc: 0.86\n",
      "Iteration 2460 - Loss: 0.43028348684310913 - Acc: 0.94\n",
      "Iteration 2470 - Loss: 0.4958925247192383 - Acc: 0.88\n",
      "Iteration 2480 - Loss: 0.4285930097103119 - Acc: 0.94\n",
      "Iteration 2490 - Loss: 0.4971439838409424 - Acc: 0.88\n",
      "Iteration 2500 - Loss: 0.39181989431381226 - Acc: 0.95\n",
      "Iteration 2510 - Loss: 0.4608898162841797 - Acc: 0.91\n",
      "Iteration 2520 - Loss: 0.4483666718006134 - Acc: 0.92\n",
      "Iteration 2530 - Loss: 0.4427669048309326 - Acc: 0.92\n",
      "Iteration 2540 - Loss: 0.4567369818687439 - Acc: 0.91\n",
      "Iteration 2550 - Loss: 0.4902864098548889 - Acc: 0.88\n",
      "Iteration 2560 - Loss: 0.36716872453689575 - Acc: 0.97\n",
      "Iteration 2570 - Loss: 0.43536093831062317 - Acc: 0.95\n",
      "Iteration 2580 - Loss: 0.46072036027908325 - Acc: 0.91\n",
      "Iteration 2590 - Loss: 0.43759575486183167 - Acc: 0.92\n",
      "Iteration 2600 - Loss: 0.4047016501426697 - Acc: 0.94\n",
      "Iteration 2610 - Loss: 0.48024579882621765 - Acc: 0.89\n",
      "Iteration 2620 - Loss: 0.4379861056804657 - Acc: 0.92\n",
      "Iteration 2630 - Loss: 0.36822977662086487 - Acc: 0.97\n",
      "Iteration 2640 - Loss: 0.40775659680366516 - Acc: 0.95\n",
      "Iteration 2650 - Loss: 0.3981945216655731 - Acc: 0.94\n",
      "Iteration 2660 - Loss: 0.42833173274993896 - Acc: 0.95\n",
      "Iteration 2670 - Loss: 0.4766928255558014 - Acc: 0.91\n",
      "Iteration 2680 - Loss: 0.43459266424179077 - Acc: 0.92\n",
      "Iteration 2690 - Loss: 0.42635732889175415 - Acc: 0.97\n",
      "Iteration 2700 - Loss: 0.424282431602478 - Acc: 0.92\n",
      "Iteration 2710 - Loss: 0.4803578853607178 - Acc: 0.88\n",
      "Iteration 2720 - Loss: 0.4607437551021576 - Acc: 0.95\n",
      "Iteration 2730 - Loss: 0.3465650975704193 - Acc: 0.98\n",
      "Iteration 2740 - Loss: 0.42468664050102234 - Acc: 0.94\n",
      "Iteration 2750 - Loss: 0.4021749496459961 - Acc: 0.95\n",
      "Iteration 2760 - Loss: 0.37936368584632874 - Acc: 0.98\n",
      "Iteration 2770 - Loss: 0.5002620816230774 - Acc: 0.86\n",
      "Iteration 2780 - Loss: 0.40716612339019775 - Acc: 0.91\n",
      "Iteration 2790 - Loss: 0.4392964839935303 - Acc: 0.94\n",
      "Iteration 2800 - Loss: 0.40611177682876587 - Acc: 0.97\n",
      "Iteration 2810 - Loss: 0.3963060975074768 - Acc: 0.95\n",
      "Iteration 2820 - Loss: 0.4594370424747467 - Acc: 0.89\n",
      "Iteration 2830 - Loss: 0.5026469826698303 - Acc: 0.83\n",
      "Iteration 2840 - Loss: 0.46895092725753784 - Acc: 0.89\n",
      "Iteration 2850 - Loss: 0.4227297902107239 - Acc: 0.91\n",
      "Iteration 2860 - Loss: 0.46134936809539795 - Acc: 0.86\n",
      "Iteration 2870 - Loss: 0.4341847002506256 - Acc: 0.92\n",
      "Iteration 2880 - Loss: 0.44959428906440735 - Acc: 0.91\n",
      "Iteration 2890 - Loss: 0.45398828387260437 - Acc: 0.91\n",
      "Iteration 2900 - Loss: 0.4518437385559082 - Acc: 0.91\n",
      "Iteration 2910 - Loss: 0.4398396611213684 - Acc: 0.92\n",
      "Iteration 2920 - Loss: 0.4439651668071747 - Acc: 0.89\n",
      "Iteration 2930 - Loss: 0.48522961139678955 - Acc: 0.91\n",
      "Iteration 2940 - Loss: 0.41255539655685425 - Acc: 0.94\n",
      "Iteration 2950 - Loss: 0.4387006461620331 - Acc: 0.92\n",
      "Iteration 2960 - Loss: 0.41705530881881714 - Acc: 0.95\n",
      "Iteration 2970 - Loss: 0.43724578619003296 - Acc: 0.92\n",
      "Iteration 2980 - Loss: 0.4211277961730957 - Acc: 0.92\n",
      "Iteration 2990 - Loss: 0.49018821120262146 - Acc: 0.92\n",
      "Iteration 3000 - Loss: 0.41777315735816956 - Acc: 0.97\n",
      "Iteration 3010 - Loss: 0.4549691379070282 - Acc: 0.91\n",
      "Iteration 3020 - Loss: 0.4658103287220001 - Acc: 0.88\n",
      "Iteration 3030 - Loss: 0.4904913008213043 - Acc: 0.88\n",
      "Iteration 3040 - Loss: 0.3991769552230835 - Acc: 0.92\n",
      "Iteration 3050 - Loss: 0.41793686151504517 - Acc: 0.91\n",
      "Iteration 3060 - Loss: 0.4364790916442871 - Acc: 0.92\n",
      "Iteration 3070 - Loss: 0.4985237717628479 - Acc: 0.88\n",
      "Iteration 3080 - Loss: 0.43970680236816406 - Acc: 0.89\n",
      "Iteration 3090 - Loss: 0.3846454918384552 - Acc: 0.97\n",
      "Iteration 3100 - Loss: 0.3968700170516968 - Acc: 0.91\n",
      "Iteration 3110 - Loss: 0.4821791648864746 - Acc: 0.89\n",
      "Iteration 3120 - Loss: 0.38756152987480164 - Acc: 0.97\n",
      "Iteration 3130 - Loss: 0.41976603865623474 - Acc: 0.94\n",
      "Iteration 3140 - Loss: 0.4659977853298187 - Acc: 0.89\n",
      "Iteration 3150 - Loss: 0.43190330266952515 - Acc: 0.92\n",
      "Iteration 3160 - Loss: 0.3885032832622528 - Acc: 0.98\n",
      "Iteration 3170 - Loss: 0.38127589225769043 - Acc: 0.94\n",
      "Iteration 3180 - Loss: 0.44845646619796753 - Acc: 0.94\n",
      "Iteration 3190 - Loss: 0.4315986633300781 - Acc: 0.92\n",
      "Iteration 3200 - Loss: 0.4658213257789612 - Acc: 0.86\n",
      "Iteration 3210 - Loss: 0.4418221116065979 - Acc: 0.91\n",
      "Iteration 3220 - Loss: 0.41086068749427795 - Acc: 0.94\n",
      "Iteration 3230 - Loss: 0.423916220664978 - Acc: 0.92\n",
      "Iteration 3240 - Loss: 0.42631369829177856 - Acc: 0.95\n",
      "Iteration 3250 - Loss: 0.4302373230457306 - Acc: 0.92\n",
      "Iteration 3260 - Loss: 0.3985864222049713 - Acc: 0.95\n",
      "Iteration 3270 - Loss: 0.4412434995174408 - Acc: 0.91\n",
      "Iteration 3280 - Loss: 0.43856579065322876 - Acc: 0.92\n",
      "Iteration 3290 - Loss: 0.3917699158191681 - Acc: 0.95\n",
      "Iteration 3300 - Loss: 0.43088212609291077 - Acc: 0.94\n",
      "Iteration 3310 - Loss: 0.4325433075428009 - Acc: 0.89\n",
      "Iteration 3320 - Loss: 0.41577398777008057 - Acc: 0.92\n",
      "Iteration 3330 - Loss: 0.42636433243751526 - Acc: 0.92\n",
      "Iteration 3340 - Loss: 0.3821104168891907 - Acc: 0.95\n",
      "Iteration 3350 - Loss: 0.3621649742126465 - Acc: 0.97\n",
      "Iteration 3360 - Loss: 0.3560881018638611 - Acc: 0.98\n",
      "Iteration 3370 - Loss: 0.5313920974731445 - Acc: 0.88\n",
      "Iteration 3380 - Loss: 0.3847172260284424 - Acc: 0.98\n",
      "Iteration 3390 - Loss: 0.43674594163894653 - Acc: 0.86\n",
      "Iteration 3400 - Loss: 0.4339711368083954 - Acc: 0.97\n",
      "Iteration 3410 - Loss: 0.45581233501434326 - Acc: 0.92\n",
      "Iteration 3420 - Loss: 0.42766278982162476 - Acc: 0.94\n",
      "Iteration 3430 - Loss: 0.4750204384326935 - Acc: 0.86\n",
      "Iteration 3440 - Loss: 0.4109300374984741 - Acc: 0.95\n",
      "Iteration 3450 - Loss: 0.4591636061668396 - Acc: 0.92\n",
      "Iteration 3460 - Loss: 0.420036643743515 - Acc: 0.95\n",
      "Iteration 3470 - Loss: 0.43792524933815 - Acc: 0.91\n",
      "Iteration 3480 - Loss: 0.45299386978149414 - Acc: 0.91\n",
      "Iteration 3490 - Loss: 0.42360803484916687 - Acc: 0.92\n",
      "Iteration 3500 - Loss: 0.4693189263343811 - Acc: 0.89\n",
      "Iteration 3510 - Loss: 0.373276948928833 - Acc: 0.95\n",
      "Iteration 3520 - Loss: 0.37509793043136597 - Acc: 0.97\n",
      "Iteration 3530 - Loss: 0.4490070939064026 - Acc: 0.88\n",
      "Iteration 3540 - Loss: 0.4290164113044739 - Acc: 0.92\n",
      "Iteration 3550 - Loss: 0.35440316796302795 - Acc: 0.98\n",
      "Iteration 3560 - Loss: 0.40380406379699707 - Acc: 0.91\n",
      "Iteration 3570 - Loss: 0.3973991870880127 - Acc: 0.94\n",
      "Iteration 3580 - Loss: 0.3839659094810486 - Acc: 0.92\n",
      "Iteration 3590 - Loss: 0.38640737533569336 - Acc: 0.98\n",
      "Iteration 3600 - Loss: 0.4218769669532776 - Acc: 0.92\n",
      "Iteration 3610 - Loss: 0.4368482530117035 - Acc: 0.89\n",
      "Iteration 3620 - Loss: 0.4284224510192871 - Acc: 0.92\n",
      "Iteration 3630 - Loss: 0.39726564288139343 - Acc: 0.91\n",
      "Iteration 3640 - Loss: 0.43760037422180176 - Acc: 0.91\n",
      "Iteration 3650 - Loss: 0.35547947883605957 - Acc: 0.97\n",
      "Iteration 3660 - Loss: 0.4004444479942322 - Acc: 0.91\n",
      "Iteration 3670 - Loss: 0.36449846625328064 - Acc: 0.97\n",
      "Iteration 3680 - Loss: 0.3813207745552063 - Acc: 0.95\n",
      "Iteration 3690 - Loss: 0.34540438652038574 - Acc: 0.98\n",
      "Iteration 3700 - Loss: 0.369388610124588 - Acc: 0.97\n",
      "Iteration 3710 - Loss: 0.43334153294563293 - Acc: 0.92\n",
      "Iteration 3720 - Loss: 0.3848741948604584 - Acc: 0.95\n",
      "Iteration 3730 - Loss: 0.3703048825263977 - Acc: 0.97\n",
      "Iteration 3740 - Loss: 0.423119455575943 - Acc: 0.88\n",
      "Iteration 3750 - Loss: 0.3476846516132355 - Acc: 0.95\n",
      "Iteration 3760 - Loss: 0.384816437959671 - Acc: 0.98\n",
      "Iteration 3770 - Loss: 0.45462319254875183 - Acc: 0.88\n",
      "Iteration 3780 - Loss: 0.4134719967842102 - Acc: 0.94\n",
      "Iteration 3790 - Loss: 0.43816909193992615 - Acc: 0.91\n",
      "Iteration 3800 - Loss: 0.3772282004356384 - Acc: 0.91\n",
      "Iteration 3810 - Loss: 0.40664514899253845 - Acc: 0.92\n",
      "Iteration 3820 - Loss: 0.41218340396881104 - Acc: 0.94\n",
      "Iteration 3830 - Loss: 0.4416581690311432 - Acc: 0.91\n",
      "Iteration 3840 - Loss: 0.37295806407928467 - Acc: 0.95\n",
      "Iteration 3850 - Loss: 0.40895992517471313 - Acc: 0.92\n",
      "Iteration 3860 - Loss: 0.38228315114974976 - Acc: 0.97\n",
      "Iteration 3870 - Loss: 0.46089014410972595 - Acc: 0.88\n",
      "Iteration 3880 - Loss: 0.42752742767333984 - Acc: 0.88\n",
      "Iteration 3890 - Loss: 0.4368514120578766 - Acc: 0.89\n",
      "Iteration 3900 - Loss: 0.4227645695209503 - Acc: 0.92\n",
      "Iteration 3910 - Loss: 0.41233277320861816 - Acc: 0.92\n",
      "Iteration 3920 - Loss: 0.3480689823627472 - Acc: 0.95\n",
      "Iteration 3930 - Loss: 0.4153902530670166 - Acc: 0.91\n",
      "Iteration 3940 - Loss: 0.3331144452095032 - Acc: 0.97\n",
      "Iteration 3950 - Loss: 0.38536468148231506 - Acc: 0.95\n",
      "Iteration 3960 - Loss: 0.3484876751899719 - Acc: 0.98\n",
      "Iteration 3970 - Loss: 0.4454469382762909 - Acc: 0.89\n",
      "Iteration 3980 - Loss: 0.38584616780281067 - Acc: 0.94\n",
      "Iteration 3990 - Loss: 0.3932622969150543 - Acc: 0.97\n",
      "Iteration 4000 - Loss: 0.42297565937042236 - Acc: 0.89\n",
      "Iteration 4010 - Loss: 0.41911906003952026 - Acc: 0.92\n",
      "Iteration 4020 - Loss: 0.4648447334766388 - Acc: 0.89\n",
      "Iteration 4030 - Loss: 0.4312452971935272 - Acc: 0.92\n",
      "Iteration 4040 - Loss: 0.41562366485595703 - Acc: 0.92\n",
      "Iteration 4050 - Loss: 0.39457619190216064 - Acc: 0.95\n",
      "Iteration 4060 - Loss: 0.4260967969894409 - Acc: 0.92\n",
      "Iteration 4070 - Loss: 0.5034830570220947 - Acc: 0.92\n",
      "Iteration 4080 - Loss: 0.4190792441368103 - Acc: 0.92\n",
      "Iteration 4090 - Loss: 0.3538980782032013 - Acc: 0.97\n",
      "Iteration 4100 - Loss: 0.3972707986831665 - Acc: 0.97\n",
      "Iteration 4110 - Loss: 0.4403845965862274 - Acc: 0.92\n",
      "Iteration 4120 - Loss: 0.38456735014915466 - Acc: 0.92\n",
      "Iteration 4130 - Loss: 0.43154817819595337 - Acc: 0.88\n",
      "Iteration 4140 - Loss: 0.44910141825675964 - Acc: 0.89\n",
      "Iteration 4150 - Loss: 0.44392311573028564 - Acc: 0.91\n",
      "Iteration 4160 - Loss: 0.3583427369594574 - Acc: 0.97\n",
      "Iteration 4170 - Loss: 0.346990704536438 - Acc: 0.95\n",
      "Iteration 4180 - Loss: 0.4491880536079407 - Acc: 0.91\n",
      "Iteration 4190 - Loss: 0.3684465289115906 - Acc: 0.94\n",
      "Iteration 4200 - Loss: 0.39781248569488525 - Acc: 0.92\n",
      "Iteration 4210 - Loss: 0.4200229346752167 - Acc: 0.91\n",
      "Iteration 4220 - Loss: 0.34597209095954895 - Acc: 0.98\n",
      "Iteration 4230 - Loss: 0.3525407910346985 - Acc: 0.95\n",
      "Iteration 4240 - Loss: 0.42050600051879883 - Acc: 0.91\n",
      "Iteration 4250 - Loss: 0.38947033882141113 - Acc: 0.94\n",
      "Iteration 4260 - Loss: 0.435884952545166 - Acc: 0.97\n",
      "Iteration 4270 - Loss: 0.31807994842529297 - Acc: 0.97\n",
      "Iteration 4280 - Loss: 0.41219621896743774 - Acc: 0.91\n",
      "Iteration 4290 - Loss: 0.4685310125350952 - Acc: 0.86\n",
      "Iteration 4300 - Loss: 0.4426218569278717 - Acc: 0.88\n",
      "Iteration 4310 - Loss: 0.4451865553855896 - Acc: 0.92\n",
      "Iteration 4320 - Loss: 0.33179885149002075 - Acc: 0.97\n",
      "Iteration 4330 - Loss: 0.39206835627555847 - Acc: 0.94\n",
      "Iteration 4340 - Loss: 0.38836440443992615 - Acc: 0.94\n",
      "Iteration 4350 - Loss: 0.36635351181030273 - Acc: 0.97\n",
      "Iteration 4360 - Loss: 0.43251922726631165 - Acc: 0.92\n",
      "Iteration 4370 - Loss: 0.44704848527908325 - Acc: 0.89\n",
      "Iteration 4380 - Loss: 0.4592028856277466 - Acc: 0.86\n",
      "Iteration 4390 - Loss: 0.3814351558685303 - Acc: 0.97\n",
      "Iteration 4400 - Loss: 0.35343778133392334 - Acc: 0.97\n",
      "Iteration 4410 - Loss: 0.3860498070716858 - Acc: 0.95\n",
      "Iteration 4420 - Loss: 0.41500580310821533 - Acc: 0.94\n",
      "Iteration 4430 - Loss: 0.3638339340686798 - Acc: 0.94\n",
      "Iteration 4440 - Loss: 0.39931511878967285 - Acc: 0.91\n",
      "Iteration 4450 - Loss: 0.3630654215812683 - Acc: 0.95\n",
      "Iteration 4460 - Loss: 0.3629515469074249 - Acc: 0.97\n",
      "Iteration 4470 - Loss: 0.4027400612831116 - Acc: 0.91\n",
      "Iteration 4480 - Loss: 0.44306549429893494 - Acc: 0.84\n",
      "Iteration 4490 - Loss: 0.4059264361858368 - Acc: 0.92\n",
      "Iteration 4500 - Loss: 0.48264390230178833 - Acc: 0.86\n",
      "Iteration 4510 - Loss: 0.4059624671936035 - Acc: 0.92\n",
      "Iteration 4520 - Loss: 0.4134630560874939 - Acc: 0.89\n",
      "Iteration 4530 - Loss: 0.49273768067359924 - Acc: 0.81\n",
      "Iteration 4540 - Loss: 0.4774650037288666 - Acc: 0.91\n",
      "Iteration 4550 - Loss: 0.3673149645328522 - Acc: 0.97\n",
      "Iteration 4560 - Loss: 0.4825406074523926 - Acc: 0.91\n",
      "Iteration 4570 - Loss: 0.37041565775871277 - Acc: 0.95\n",
      "Iteration 4580 - Loss: 0.4666116237640381 - Acc: 0.91\n",
      "Iteration 4590 - Loss: 0.40415987372398376 - Acc: 0.92\n",
      "Iteration 4600 - Loss: 0.44983598589897156 - Acc: 0.91\n",
      "Iteration 4610 - Loss: 0.486384779214859 - Acc: 0.88\n",
      "Iteration 4620 - Loss: 0.3914361596107483 - Acc: 0.97\n",
      "Iteration 4630 - Loss: 0.36707890033721924 - Acc: 0.97\n",
      "Iteration 4640 - Loss: 0.38825079798698425 - Acc: 0.97\n",
      "Iteration 4650 - Loss: 0.3982921838760376 - Acc: 0.94\n",
      "Iteration 4660 - Loss: 0.3725087642669678 - Acc: 0.92\n",
      "Iteration 4670 - Loss: 0.4078255295753479 - Acc: 0.95\n",
      "Iteration 4680 - Loss: 0.39169034361839294 - Acc: 0.92\n",
      "Iteration 4690 - Loss: 0.37273287773132324 - Acc: 0.94\n",
      "Iteration 4700 - Loss: 0.41445043683052063 - Acc: 0.88\n",
      "Iteration 4710 - Loss: 0.43379509449005127 - Acc: 0.89\n",
      "Iteration 4720 - Loss: 0.4565616846084595 - Acc: 0.89\n",
      "Iteration 4730 - Loss: 0.47372573614120483 - Acc: 0.84\n",
      "Iteration 4740 - Loss: 0.3780055642127991 - Acc: 0.92\n",
      "Iteration 4750 - Loss: 0.439413845539093 - Acc: 0.94\n",
      "Iteration 4760 - Loss: 0.40912705659866333 - Acc: 0.92\n",
      "Iteration 4770 - Loss: 0.4930921494960785 - Acc: 0.89\n",
      "Iteration 4780 - Loss: 0.3798344135284424 - Acc: 0.94\n",
      "Iteration 4790 - Loss: 0.3878929913043976 - Acc: 0.92\n",
      "Iteration 4800 - Loss: 0.39591723680496216 - Acc: 0.97\n",
      "Iteration 4810 - Loss: 0.4132390022277832 - Acc: 0.94\n",
      "Iteration 4820 - Loss: 0.3247080445289612 - Acc: 0.98\n",
      "Iteration 4830 - Loss: 0.44640421867370605 - Acc: 0.89\n",
      "Iteration 4840 - Loss: 0.360958069562912 - Acc: 0.95\n",
      "Iteration 4850 - Loss: 0.40346449613571167 - Acc: 0.92\n",
      "Iteration 4860 - Loss: 0.37280356884002686 - Acc: 0.94\n",
      "Iteration 4870 - Loss: 0.4258127510547638 - Acc: 0.91\n",
      "Iteration 4880 - Loss: 0.39405280351638794 - Acc: 0.92\n",
      "Iteration 4890 - Loss: 0.39895620942115784 - Acc: 0.95\n",
      "Iteration 4900 - Loss: 0.4319228529930115 - Acc: 0.94\n",
      "Iteration 4910 - Loss: 0.3992498815059662 - Acc: 0.92\n",
      "Iteration 4920 - Loss: 0.4582948684692383 - Acc: 0.89\n",
      "Iteration 4930 - Loss: 0.3863552212715149 - Acc: 0.94\n",
      "Iteration 4940 - Loss: 0.4290807247161865 - Acc: 0.89\n",
      "Iteration 4950 - Loss: 0.47642531991004944 - Acc: 0.89\n",
      "Iteration 4960 - Loss: 0.4525392949581146 - Acc: 0.91\n",
      "Iteration 4970 - Loss: 0.39356982707977295 - Acc: 0.94\n",
      "Iteration 4980 - Loss: 0.4102843403816223 - Acc: 0.92\n",
      "Iteration 4990 - Loss: 0.36710065603256226 - Acc: 0.97\n",
      "Iteration 5000 - Loss: 0.37223851680755615 - Acc: 0.91\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, num_iterations + 1):\n",
    "    x, y = get_train_data(batch_size, train_images, train_labels, 56)\n",
    "    loss = siamese_net.train_on_batch(x, y)\n",
    "    if i % evaluateEvery == 0:\n",
    "        print('Iteration', i, '- Loss:',loss[0],'- Acc:', round(loss[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing ...\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "27/27 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "27/27 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "25/25 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "28/28 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "24/24 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "18/18 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "27/27 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "28/28 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "14/14 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "15/15 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "Accuracy:  0.2534142640364188\n",
      "======= NL Autoencoder method: Finished =======\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "total = 0\n",
    "(test_images, test_labels) = resize_images(df_test['image'], 56), df_test['label'].to_numpy()\n",
    "\n",
    "print(\"Vectorizing ...\")\n",
    "for alphabet in np.unique(df_test['alphabet']):\n",
    "    ind_alphabet = np.where(df_test['alphabet'] == alphabet)[0]\n",
    "    labels = test_labels[ind_alphabet]\n",
    "    images = test_images[ind_alphabet]\n",
    "    os_img, os_label, clas_img, clas_label = separate_fewshot(images, labels, n=1)\n",
    "\n",
    "    os_img = encoder.predict(os_img)\n",
    "    clas_img = encoder.predict(clas_img)\n",
    "\n",
    "    #if verbose: print(\"Learning oneshot ...\")\n",
    "    #nn = min(train, 5)\n",
    "    neigh = KNeighborsClassifier(n_neighbors = 1)\n",
    "    neigh.fit(os_img, os_label)\n",
    "\n",
    "    #if verbose: print(\"Predicting ...\")\n",
    "    pred = neigh.predict(clas_img)\n",
    "\n",
    "    matches += np.sum(pred == clas_label)\n",
    "    total += len(clas_label)\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", matches/total)\n",
    "print(\"======= NL Autoencoder method: Finished =======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
